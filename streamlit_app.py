# -*- coding: utf-8 -*-
import os, json, re, time, uuid
import streamlit as st
import pandas as pd
import numpy as np

# ì§€ë„
import folium
from folium.features import GeoJson, GeoJsonTooltip, GeoJsonPopup
from streamlit_folium import st_folium

st.set_page_config(page_title="ë””ì§€í„¸ ë…¸ë§ˆë“œ ì§€ì—­ ì¶”ì²œ ëŒ€ì‹œë³´ë“œ", layout="wide")

# ------------------------------- ê²½ë¡œ/í´ë” -------------------------------
APP_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(APP_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)
CANDIDATE_BASES = [DATA_DIR, APP_DIR, "."] # ì‹¤í–‰ íŒŒì¼ ê¸°ì¤€ í´ë” ì¶”ê°€

def build_paths():
    for base in CANDIDATE_BASES:
        fv = os.path.join(base, "20250809144224_ê´‘ì—­ë³„ ë°©ë¬¸ì ìˆ˜.csv")
        fc = os.path.join(base, "PLP_ì—…ì¢…ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv")
        ft = os.path.join(base, "PLP_ìœ í˜•ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv")
        # ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ëŒ€ì‹ , ê° íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ëŠ” ë¡œë”© í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬
    return (
        os.path.join(CANDIDATE_BASES[0], "20250809144224_ê´‘ì—­ë³„ ë°©ë¬¸ì ìˆ˜.csv"),
        os.path.join(CANDIDATE_BASES[0], "PLP_ì—…ì¢…ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv"),
        os.path.join(CANDIDATE_BASES[0], "PLP_ìœ í˜•ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv"),
    )

file_visitors, file_search_cat, file_search_type = build_paths()

def resolve_geojson_path():
    env_path = os.environ.get("GEOJSON_PATH", "").strip()
    if env_path and os.path.exists(env_path):
        return env_path
    for p in [
        os.path.join(DATA_DIR, "korea_provinces.geojson"),
        os.path.join(DATA_DIR, "KOREA_GEOJSON.geojson"),
        os.path.join(APP_DIR,  "korea_provinces.geojson"),
        os.path.join(APP_DIR,  "KOREA_GEOJSON.geojson"),
    ]:
        if os.path.exists(p):
            return p
    return ""

KOREA_GEOJSON = resolve_geojson_path()
GEO_PROP_KEYS = ["name", "CTPRVN_NM", "ADM1_KOR_NM", "sido_nm", "SIG_KOR_NM", "NAME_1"]

# ---- ì¸í”„ë¼ í´ë”/ZIP ìë™ íƒìƒ‰ ----
INFRA_DIR_NAME  = "ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€(ìƒê¶Œ)ì •ë³´_20250630"
INFRA_ZIP_NAME  = "ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€(ìƒê¶Œ)ì •ë³´_20250630.zip"

def resolve_infra_sources():
    for base in CANDIDATE_BASES:
        d = os.path.join(base, INFRA_DIR_NAME)
        if os.path.isdir(d):
            csvs = []
            for r, _, files in os.walk(d):
                for f in files:
                    if f.lower().endswith(".csv"):
                        csvs.append(os.path.join(r, f))
            if csvs:
                return {"mode": "dir", "paths": csvs}
    for base in CANDIDATE_BASES:
        z = os.path.join(base, INFRA_ZIP_NAME)
        if os.path.exists(z):
            return {"mode": "zip", "paths": [z]}
    return {"mode": "none", "paths": []}

# ---------------------------- í‘œì¤€/ì•½ì‹ ì§€ì—­ëª… ----------------------------
TWOCHAR_MAP = {"ì„œìš¸":"ì„œìš¸","ë¶€ì‚°":"ë¶€ì‚°","ëŒ€êµ¬":"ëŒ€êµ¬","ì¸ì²œ":"ì¸ì²œ","ê´‘ì£¼":"ê´‘ì£¼","ëŒ€ì „":"ëŒ€ì „","ìš¸ì‚°":"ìš¸ì‚°","ì„¸ì¢…":"ì„¸ì¢…",
               "ê²½ê¸°":"ê²½ê¸°","ê°•ì›":"ê°•ì›","ì¶©ë¶":"ì¶©ë¶","ì¶©ë‚¨":"ì¶©ë‚¨","ì „ë¶":"ì „ë¶","ì „ë‚¨":"ì „ë‚¨","ê²½ë¶":"ê²½ë¶","ê²½ë‚¨":"ê²½ë‚¨","ì œì£¼":"ì œì£¼"}

def to_twochar(s: str) -> str:
    s = str(s)
    if s.startswith("ì „ë¼ë‚¨"): return "ì „ë‚¨"
    if s.startswith("ì „ë¼ë¶"): return "ì „ë¶"
    if s.startswith("ê²½ìƒë‚¨"): return "ê²½ë‚¨"
    if s.startswith("ê²½ìƒë¶"): return "ê²½ë¶"
    if s.startswith("ì¶©ì²­ë‚¨"): return "ì¶©ë‚¨"
    if s.startswith("ì¶©ì²­ë¶"): return "ì¶©ë¶"
    s = re.sub(r"(íŠ¹ë³„ìì¹˜ë„|íŠ¹ë³„ìì¹˜ì‹œ|íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ìì¹˜ë„|ìì¹˜ì‹œ|ë„|ì‹œ)$","", s)
    return TWOCHAR_MAP.get(s, s[:2])

REGION_COORDS = {
    "ì„œìš¸": (37.5665, 126.9780), "ë¶€ì‚°": (35.1796, 129.0756), "ëŒ€êµ¬": (35.8714, 128.6014),
    "ì¸ì²œ": (37.4563, 126.7052), "ê´‘ì£¼": (35.1595, 126.8526), "ëŒ€ì „": (36.3504, 127.3845),
    "ìš¸ì‚°": (35.5384, 129.3114), "ì„¸ì¢…": (36.4800, 127.2890), "ê²½ê¸°": (37.4138, 127.5183),
    "ê°•ì›": (37.8228, 128.1555), "ì¶©ë¶": (36.6357, 127.4913), "ì¶©ë‚¨": (36.5184, 126.8000),
    "ì „ë¶": (35.7175, 127.1530), "ì „ë‚¨": (34.8679, 126.9910), "ê²½ë¶": (36.4919, 128.8889),
    "ê²½ë‚¨": (35.4606, 128.2132), "ì œì£¼": (33.4996, 126.5312),
}

def normalize_region_name(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\(.*?\)", "", str(s))
    s = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", s).strip()
    for a in ["íŠ¹ë³„ìì¹˜ë„","íŠ¹ë³„ìì¹˜ì‹œ","íŠ¹ë³„ì‹œ","ê´‘ì—­ì‹œ","ìì¹˜ë„","ìì¹˜ì‹œ","ë„","ì‹œ"]:
        s = s.replace(a, "")
    s = re.sub(r"\s+", " ", s).strip()
    return to_twochar(s)

def minmax(s):
    s = pd.to_numeric(s, errors="coerce")
    if s.notna().sum() == 0: return s.fillna(0.0)
    d = s.max() - s.min()
    return (s - s.min())/d if d>0 else s*0

# --------------------------- ì•ˆì „ GeoJSON ë¡œë” ---------------------------
@st.cache_data(show_spinner=False)
def load_geojson_safe(path: str):
    if not path or not os.path.exists(path):
        return None, "missing_path"
    try:
        if os.path.getsize(path) == 0:
            return None, "empty_file"
    except Exception:
        pass
    for enc in ("utf-8","utf-8-sig","utf-16","utf-16-le","utf-16-be","cp949","euc-kr","latin-1"):
        try:
            with open(path,"r",encoding=enc,errors="strict") as f:
                txt=f.read().strip()
            if not txt: return None, f"empty_text({enc})"
            if txt.lstrip().startswith(("var ","const ","let ")):
                m=re.search(r"\{.*\}\s*;?\s*$", txt, flags=re.S)
                if m: txt=m.group(0)
            gj=json.loads(txt)
            if not isinstance(gj,dict): continue
            if gj.get("type")=="Topology": return None, "topojson_not_supported"
            if gj.get("type")!="FeatureCollection" or not isinstance(gj.get("features"),list):
                continue
            return gj, None
        except Exception:
            continue
    return None, "parse_error"

# ----------------------------- CSV ë¡œë” -----------------------------
NEEDED_VIS_COLS = ["ê´‘ì—­ì§€ìì²´ëª…", "ê¸°ì´ˆì§€ìì²´ ë°©ë¬¸ì ìˆ˜"]

@st.cache_data(show_spinner=False)
def read_csv_forgiving(path, usecols=None, dtype=None):
    if not path or not os.path.exists(path):
        return pd.DataFrame()
    for enc in ["utf-8","cp949","euc-kr","utf-8-sig","latin1"]:
        try:
            return pd.read_csv(path, encoding=enc, usecols=usecols, dtype=dtype, low_memory=False)
        except Exception:
            continue
    try:
        return pd.read_csv(path, usecols=usecols, dtype=dtype, low_memory=False)
    except Exception:
        return pd.DataFrame()

@st.cache_data(show_spinner=False)
def read_visitors():
    return read_csv_forgiving(file_visitors, usecols=NEEDED_VIS_COLS, dtype={"ê´‘ì—­ì§€ìì²´ëª…":"string"})

# ----- (ê·¸ë˜í”„ìš©) ê²€ìƒ‰ê±´ìˆ˜ íŒŒì¼ ë¡œë”: ìœ ì—° ì»¬ëŸ¼ ê°ì§€ -----
@st.cache_data(show_spinner=False)
def load_search_counts(path):
    df = read_csv_forgiving(path)
    if df.empty: return pd.DataFrame(), (None, None, None)

    cols={c.lower():c for c in df.columns}
    rcol=None
    for k in ["ì§€ì—­","ì‹œë„","ì‹œë„ëª…","ê´‘ì—­ì§€ìì²´","sido","region","province"]:
        if k in cols: rcol=cols[k]; break
    gcol=None
    for k in ["ì¤‘ë¶„ë¥˜","ëŒ€ë¶„ë¥˜","ì—…ì¢…","ì¹´í…Œê³ ë¦¬","ìœ í˜•","í‚¤ì›Œë“œ","ê²€ìƒ‰ì–´","í•­ëª©"]:
        if k in cols: gcol=cols[k]; break
    vcol=None
    for k in ["ê²€ìƒ‰ê±´ìˆ˜","ê²€ìƒ‰ ê±´ìˆ˜","ê²€ìƒ‰ìˆ˜","ê²€ìƒ‰ëŸ‰","count","ê±´ìˆ˜","value","í•©ê³„","ì´ê±´ìˆ˜"]:
        if k in cols: vcol=cols[k]; break
    if vcol: df[vcol]=pd.to_numeric(df[vcol], errors="coerce")
    return df, (rcol,gcol,vcol)

# ----------------------- í•µì‹¬ ì§€í‘œ ê³„ì‚° -----------------------
vis = read_visitors()

# --- ì˜¤ë¥˜ ìˆ˜ì • ë¶€ë¶„ ì‹œì‘ ---
# ë°©ë¬¸ì íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆì •ì„± ê°•í™”
vis_region = pd.DataFrame()
if not vis.empty:
    vis_region = (vis.groupby("ê´‘ì—­ì§€ìì²´ëª…", as_index=False)["ê¸°ì´ˆì§€ìì²´ ë°©ë¬¸ì ìˆ˜"]
                   .sum()
                   .rename(columns={"ê¸°ì´ˆì§€ìì²´ ë°©ë¬¸ì ìˆ˜":"ë°©ë¬¸ììˆ˜_í•©ê³„"}))
    total_visitors = max(vis_region["ë°©ë¬¸ììˆ˜_í•©ê³„"].sum(), 1)
    vis_region["ë°©ë¬¸ì_ì ìœ ìœ¨"] = vis_region["ë°©ë¬¸ììˆ˜_í•©ê³„"] / total_visitors
    vis_region["ì§€ì—­_norm"] = vis_region["ê´‘ì—­ì§€ìì²´ëª…"].map(normalize_region_name)
else:
    # st.warningì„ ì‚¬ìš©í•˜ì—¬ ì•± í™”ë©´ì— ê²½ê³  ë©”ì‹œì§€ í‘œì‹œ
    st.warning(f"ë°©ë¬¸ì ìˆ˜ ë°ì´í„° íŒŒì¼({os.path.basename(file_visitors)})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°©ë¬¸ì ê´€ë ¨ ì ìˆ˜ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
    all_regions = list(REGION_COORDS.keys())
    vis_region = pd.DataFrame({
        "ê´‘ì—­ì§€ìì²´ëª…": all_regions,
        "ë°©ë¬¸ììˆ˜_í•©ê³„": 0,
        "ë°©ë¬¸ì_ì ìœ ìœ¨": 0,
        "ì§€ì—­_norm": [normalize_region_name(r) for r in all_regions]
    })
# --- ì˜¤ë¥˜ ìˆ˜ì • ë¶€ë¶„ ë ---

# ê¸°ë³¸ ë©”íŠ¸ë¦­: ë°©ë¬¸ìë§Œ ë°˜ì˜
metrics_map = vis_region.copy()
coords_df = pd.DataFrame([{"ì§€ì—­_norm":k,"lat":v[0],"lon":v[1]} for k,v in REGION_COORDS.items()])
metrics_map = metrics_map.merge(coords_df, on="ì§€ì—­_norm", how="left")
metrics_map["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] = minmax(metrics_map["ë°©ë¬¸ì_ì ìœ ìœ¨"].fillna(0))
metrics_map["NSI_base"] = metrics_map["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].fillna(0)

# ==================== ì¸í”„ë¼ ì§€í‘œ(ìƒê°€ í´ë”/ZIP) í†µí•© ====================
@st.cache_data(show_spinner=True)
def build_infra_from_sources(sources):
    import io, zipfile
    required = {"ì‹œë„ëª…","ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…","ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…","í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…"}
    dfs = []
    if sources["mode"] == "dir":
        for path in sources["paths"]:
            df=read_csv_forgiving(path)
            if df.empty or not required.issubset(set(df.columns)):
                continue
            dfs.append(df[list(required)].copy())
    elif sources["mode"] == "zip":
        zpath = sources["paths"][0]
        with zipfile.ZipFile(zpath,"r") as z:
            for name in z.namelist():
                if not name.lower().endswith(".csv"): continue
                raw=z.read(name)
                df=read_csv_forgiving(io.BytesIO(raw))
                if df.empty or not required.issubset(set(df.columns)):
                    continue
                dfs.append(df[list(required)].copy())
    if not dfs: return pd.DataFrame()

    df=pd.concat(dfs, ignore_index=True)
    for c in required: df[c]=df[c].astype(str).str.strip()

    mid=df["ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…"].astype(str)
    sub=df["ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…"].astype(str)
    std=df["í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…"].astype(str)

    m_cafe      = (sub.str.contains("ì¹´í˜")) | (std.str.contains("ì»¤í”¼ ì „ë¬¸ì "))
    m_conv      = (sub.str.contains("í¸ì˜ì ")) | (std.str.contains("ì²´ì¸í™” í¸ì˜ì "))
    m_laundry   = sub.str.contains("ì„¸íƒì†Œ|ë¹¨ë˜ë°©")
    m_pharmacy  = sub.str.contains("ì•½êµ­")
    m_clinic    = mid.str.contains("ì˜ì›")
    m_hospital  = mid.str.contains("ë³‘ì›") | m_clinic | sub.str.contains("ì¹˜ê³¼ì˜ì›|í•œì˜ì›")
    m_library   = mid.str.contains("ë„ì„œê´€|ì‚¬ì ì§€")
    m_pc        = sub.str.contains("PCë°©")
    m_karaoke   = sub.str.contains("ë…¸ë˜ë°©|ë…¸ë˜ì—°ìŠµì¥")
    m_fitness   = sub.str.contains("í—¬ìŠ¤ì¥")
    m_yoga      = sub.str.contains("ìš”ê°€|í•„ë¼í…ŒìŠ¤")

    df["sido_norm"]=df["ì‹œë„ëª…"].map(normalize_region_name)
    agg=df.groupby("sido_norm").agg(
        total_places=("ì‹œë„ëª…","size"),
        cafe_count=("ì‹œë„ëª…",             lambda s:int(m_cafe.loc[s.index].sum())),
        convenience_count=("ì‹œë„ëª…",      lambda s:int(m_conv.loc[s.index].sum())),
        laundry_count=("ì‹œë„ëª…",          lambda s:int(m_laundry.loc[s.index].sum())),
        pharmacy_count=("ì‹œë„ëª…",         lambda s:int(m_pharmacy.loc[s.index].sum())),
        hospital_count=("ì‹œë„ëª…",         lambda s:int(m_hospital.loc[s.index].sum())),
        library_museum_count=("ì‹œë„ëª…",   lambda s:int(m_library.loc[s.index].sum())),
        pc_cafe_count=("ì‹œë„ëª…",          lambda s:int(m_pc.loc[s.index].sum())),
        karaoke_count=("ì‹œë„ëª…",          lambda s:int(m_karaoke.loc[s.index].sum())),
        fitness_count=("ì‹œë„ëª…",          lambda s:int(m_fitness.loc[s.index].sum())),
        yoga_pilates_count=("ì‹œë„ëª…",     lambda s:int(m_yoga.loc[s.index].sum())),
    ).reset_index()

    def per_10k(n,total): total=total.replace(0,np.nan); return (n/total)*10000

    metric_cols = [
        "cafe_count","convenience_count","laundry_count","pharmacy_count",
        "hospital_count","library_museum_count","pc_cafe_count",
        "karaoke_count","fitness_count","yoga_pilates_count"
    ]
    for col in metric_cols:
        agg[col+"_per10k"] = per_10k(agg[col].astype(float), agg["total_places"].astype(float)).round(3)
        v=agg[col].astype(float); rng=v.max()-v.min()
        agg[col+"_norm"] = ((v-v.min())/rng).fillna(0).round(4) if rng>0 else v*0
    return agg

# ==================== KTX ë°ì´í„° ë¡œë“œ ====================
def find_optional_file(names):
    for base in CANDIDATE_BASES:
        for n in names:
            p=os.path.join(base,n)
            if os.path.exists(p): return p
    return ""

@st.cache_data(show_spinner=False)
def load_ktx_counts(path):
    df = read_csv_forgiving(path)
    if df.empty: return pd.DataFrame()

    cols={c.lower():c for c in df.columns}
    sido_col=cols.get("ì‹œë„") or cols.get("ì‹œë„ëª…") or cols.get("ê´‘ì—­ì§€ìì²´")
    if not sido_col:
        addr_col=None
        addr_keys = ["ì—­ì£¼ì†Œ","ì£¼ì†Œ","ì†Œì¬ì§€","ì†Œì¬ì§€ì£¼ì†Œ","ì—­ì‚¬ì£¼ì†Œ","ì§€ë²ˆì£¼ì†Œ","ë„ë¡œëª…ì£¼ì†Œ","ì—­ì‚¬ ë„ë¡œëª…ì£¼ì†Œ"]
        for key in addr_keys:
            if key in cols: addr_col = cols[key]; break
        if not addr_col:
            for key in addr_keys:
                 for c in df.columns:
                    if key in c: addr_col=c; break
                 if addr_col: break
        if not addr_col: return pd.DataFrame()

        sidos=df[addr_col].astype(str)
        def ext(addr):
            m=re.match(r"(ì„œìš¸íŠ¹ë³„ì‹œ|ë¶€ì‚°ê´‘ì—­ì‹œ|ëŒ€êµ¬ê´‘ì—­ì‹œ|ì¸ì²œê´‘ì—­ì‹œ|ê´‘ì£¼ê´‘ì—­ì‹œ|ëŒ€ì „ê´‘ì—­ì‹œ|ìš¸ì‚°ê´‘ì—­ì‹œ|ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ|ì œì£¼íŠ¹ë³„ìì¹˜ë„|ê²½ê¸°ë„|ê°•ì›íŠ¹ë³„ìì¹˜ë„|ê°•ì›ë„|ì¶©ì²­ë¶ë„|ì¶©ì²­ë‚¨ë„|ì „ë¼ë¶ë„|ì „ë¼ë‚¨ë„|ê²½ìƒë¶ë„|ê²½ìƒë‚¨ë„)", addr.strip())
            return m.group(1) if m else ""
        sidos=sidos.map(ext)
    else:
        sidos=df[sido_col].astype(str)
    ktx=pd.DataFrame({"ì§€ì—­_norm":sidos.map(normalize_region_name)})
    ktx=ktx[ktx["ì§€ì—­_norm"].astype(str).str.len()>0]
    return ktx.value_counts("ì§€ì—­_norm").rename("ktx_cnt").reset_index()

@st.cache_data(show_spinner=False)
def load_coworking(path):
    df = read_csv_forgiving(path)
    if df.empty: return pd.DataFrame()

    cols_lower={c.lower():c for c in df.columns}
    sido_col=None
    for k in ["ì‹œë„","ì‹œë„ëª…","ê´‘ì—­ì§€ìì²´","sido"]:
        if k in df.columns: sido_col=k; break
        if k in cols_lower: sido_col=cols_lower[k]; break
    if not sido_col:
        addr_col=None
        for key in ["ì£¼ì†Œ","ì†Œì¬ì§€","ë„ë¡œëª…ì£¼ì†Œ","ìƒì„¸ì£¼ì†Œ","ì§€ë²ˆì£¼ì†Œ"]:
            for c in df.columns:
                if key in c: addr_col=c; break
            if addr_col: break
        if not addr_col: return pd.DataFrame()
        sidos=df[addr_col].astype(str)
        def ext(addr):
            m=re.match(r"(ì„œìš¸íŠ¹ë³„ì‹œ|ë¶€ì‚°ê´‘ì—­ì‹œ|ëŒ€êµ¬ê´‘ì—­ì‹œ|ì¸ì²œê´‘ì—­ì‹œ|ê´‘ì£¼ê´‘ì—­ì‹œ|ëŒ€ì „ê´‘ì—­ì‹œ|ìš¸ì‚°ê´‘ì—­ì‹œ|ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ|ì œì£¼íŠ¹ë³„ìì¹˜ë„|ê²½ê¸°ë„|ê°•ì›íŠ¹ë³„ìì¹˜ë„|ê°•ì›ë„|ì¶©ì²­ë¶ë„|ì¶©ì²­ë‚¨ë„|ì „ë¼ë¶ë„|ì „ë¼ë‚¨ë„|ê²½ìƒë¶ë„|ê²½ìƒë‚¨ë„)", addr.strip())
            return m.group(1) if m else ""
        sidos=sidos.map(ext)
    else:
        sidos=df[sido_col].astype(str)
    g=pd.DataFrame({"ì§€ì—­_norm":sidos.map(normalize_region_name)})
    g=g[g["ì§€ì—­_norm"]!=""]
    return g.value_counts("ì§€ì—­_norm").rename("coworking_sites").reset_index()

# ============================ UI ë ˆì´ì•„ì›ƒ ============================
st.title("ë””ì§€í„¸ ë…¸ë§ˆë“œ ì§€ì—­ ì¶”ì²œ ëŒ€ì‹œë³´ë“œ")
left, right = st.columns([2, 1])
with left:
    st.subheader("ì§€ë„ì—ì„œ ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”")

# -------- ì‚¬ì´ë“œë°” --------
st.sidebar.header("ì¶”ì²œ ì¹´í…Œê³ ë¦¬")
CATEGORY_OPTIONS = [
    "ğŸ”¥ í˜„ì¬ ì¸ê¸° ì§€ì—­",
    "ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­",
    "ğŸ› ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­",
    "ğŸ’° í•©ë¦¬ì ì¸ì¸ ë¹„ìš©",
    "ğŸš€ ë¹ ë¥¸ ì¸í„°ë„·",
]
selected_category = st.sidebar.selectbox("í•˜ë‚˜ë§Œ ì„ íƒ", CATEGORY_OPTIONS, index=0)

st.sidebar.markdown("---")
st.sidebar.caption("ì£¼ë³€ ì¸í”„ë¼ ì„ íƒ (ë³´ë„ˆìŠ¤ ì ìˆ˜)")
cb_infra_cafe    = st.sidebar.checkbox("â˜• ì¹´í˜", value=False)
cb_infra_lib     = st.sidebar.checkbox("ğŸ›ï¸ ë„ì„œê´€", value=False)
cb_infra_conv    = st.sidebar.checkbox("ğŸª í¸ì˜ì ", value=False)
cb_infra_laundry = st.sidebar.checkbox("ğŸ§º ì„¸íƒì†Œ", value=False)
cb_infra_hos     = st.sidebar.checkbox("ğŸ¥ ë³‘Â·ì˜ì›", value=False)
cb_infra_phar    = st.sidebar.checkbox("ğŸ’Š ì•½êµ­", value=False)
cb_infra_pc      = st.sidebar.checkbox("ğŸ’» PCë°©", value=False)
cb_infra_karaoke = st.sidebar.checkbox("ğŸ¤ ë…¸ë˜ë°©", value=False)
cb_infra_fit     = st.sidebar.checkbox("ğŸ’ª í—¬ìŠ¤ì¥", value=False)
cb_infra_yoga    = st.sidebar.checkbox("ğŸ§˜ ìš”ê°€/í•„ë¼í…ŒìŠ¤", value=False)

# ----------------- ë°ì´í„° ë¡œë“œ/ê³„ì‚° -----------------
need_infra = any([
    cb_infra_cafe, cb_infra_lib, cb_infra_conv, cb_infra_laundry,
    cb_infra_hos, cb_infra_phar, cb_infra_pc, cb_infra_karaoke,
    cb_infra_fit, cb_infra_yoga, selected_category == "ğŸ› ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­"
])
need_access = (selected_category=="ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­")
need_cowork = (selected_category=="ğŸ› ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­")

if need_infra:
    infra_sources = resolve_infra_sources()
    infra_df = build_infra_from_sources(infra_sources)
    if not infra_df.empty:
        metrics_map = metrics_map.merge(
            infra_df.add_prefix("infra__").rename(columns={"infra__sido_norm":"ì§€ì—­_norm"}),
            on="ì§€ì—­_norm", how="left"
        )
else:
    infra_df = pd.DataFrame()

if need_access:
    ktx_file_path = find_optional_file([
        "í•œêµ­ì² ë„ê³µì‚¬_KTX ë…¸ì„ ë³„ ì—­ì •ë³´_20240411.csv", "KTX_ë…¸ì„ ë³„_ì—­ì •ë³´.csv", "ktx_stations.csv"
    ])
    ktx_df = load_ktx_counts(ktx_file_path)

    if not ktx_df.empty:
        metrics_map = metrics_map.merge(ktx_df, on="ì§€ì—­_norm", how="left")
        metrics_map["ktx_cnt"] = metrics_map["ktx_cnt"].fillna(0)
        metrics_map["access_score"] = minmax(metrics_map["ktx_cnt"])
    else:
        st.warning(f"KTX ì—­ ì •ë³´ íŒŒì¼({os.path.basename(ktx_file_path)})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        metrics_map["ktx_cnt"] = 0
        metrics_map["access_score"] = 0.0

if need_cowork:
    cowork_files = [
        "KC_CNRS_OFFM_FCLTY_DATA_2023.csv","ê³µìœ ì˜¤í”¼ìŠ¤.csv","coworking_sites.csv",
        "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ì§„í¥ê³µë‹¨_ê³µìœ ì˜¤í”¼ìŠ¤_ìš´ì˜í˜„í™©.csv","í•œêµ­ë¬¸í™”ì •ë³´ì›_ì „êµ­ê³µìœ ì˜¤í”¼ìŠ¤ì‹œì„¤ë°ì´í„°.csv","ì „êµ­_ê³µìœ _ì˜¤í”¼ìŠ¤_ì‹œì„¤_ë°ì´í„°.csv"
    ]
    cw_file = find_optional_file(cowork_files)
    cow_df = load_coworking(cw_file)
    if not cow_df.empty:
        if need_infra and "infra__total_places" in metrics_map.columns:
            base = metrics_map[["ì§€ì—­_norm","infra__total_places"]]
            cow = cow_df.merge(base, on="ì§€ì—­_norm", how="left")
            cow["cowork_per10k"] = (cow["coworking_sites"]/cow["infra__total_places"].replace(0,np.nan)*10000).round(3)
            v = pd.to_numeric(cow["cowork_per10k"], errors="coerce")
        else:
            cow = cow_df.copy()
            cow["cowork_per10k"]=np.nan
            v = pd.to_numeric(cow["coworking_sites"], errors="coerce")
        rng=(v.max()-v.min())
        cow["cowork_norm"]=((v-v.min())/rng).fillna(0) if rng>0 else (v*0)
        metrics_map = metrics_map.merge(cow[["ì§€ì—­_norm","coworking_sites","cowork_per10k","cowork_norm"]], on="ì§€ì—­_norm", how="left")

# ----------------------------- ë£°/ë³´ë„ˆìŠ¤ -----------------------------
def _compute_bonus_columns(g, selected_category):
    CAT_BONUS   = 0.15
    INFRA_BONUS = 0.10
    q_vis_hi = g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].quantile(0.70)
    q_acc_hi = g["access_score"].quantile(0.70) if "access_score" in g and g["access_score"].notna().any() else 1.0
    q_cwk_hi = g["cowork_norm"].quantile(0.70) if "cowork_norm" in g and g["cowork_norm"].notna().any() else 1.0

    bonus = np.zeros(len(g), dtype=float)

    def add_above(series, q):
        s=pd.to_numeric(series, errors="coerce").fillna(0)
        return (s - q).clip(lower=0)

    if selected_category == "ğŸ”¥ í˜„ì¬ ì¸ê¸° ì§€ì—­":
        bonus += CAT_BONUS * add_above(g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"], q_vis_hi)
    elif selected_category == "ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­" and "access_score" in g:
        bonus += CAT_BONUS * add_above(g["access_score"], q_acc_hi)
    elif selected_category == "ğŸ› ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­" and "cowork_norm" in g:
        bonus += CAT_BONUS * add_above(g["cowork_norm"], q_cwk_hi)
    elif selected_category == "ğŸ’° í•©ë¦¬ì ì¸ì¸ ë¹„ìš©" and "cost_index" in g and g["cost_index"].notna().any():
        cost = g["cost_index"]
        rng = cost.max() - cost.min()
        if rng > 0:
            normalized_cost = (cost - cost.min()) / rng
            bonus += CAT_BONUS * (1 - normalized_cost)
    elif selected_category == "ğŸš€ ë¹ ë¥¸ ì¸í„°ë„·" and "internet_mbps" in g and g["internet_mbps"].notna().any():
         bonus += CAT_BONUS * add_above(g["internet_mbps"], g["internet_mbps"].quantile(0.70))

    if cb_infra_cafe and "infra__cafe_count_norm" in g: bonus += INFRA_BONUS * g["infra__cafe_count_norm"].fillna(0)
    if cb_infra_lib and "infra__library_museum_count_norm" in g: bonus += INFRA_BONUS * g["infra__library_museum_count_norm"].fillna(0)
    if cb_infra_conv and "infra__convenience_count_norm" in g: bonus += INFRA_BONUS * g["infra__convenience_count_norm"].fillna(0)
    if cb_infra_laundry and "infra__laundry_count_norm" in g: bonus += INFRA_BONUS * g["infra__laundry_count_norm"].fillna(0)
    if cb_infra_hos and "infra__hospital_count_norm" in g: bonus += INFRA_BONUS * g["infra__hospital_count_norm"].fillna(0)
    if cb_infra_phar and "infra__pharmacy_count_norm" in g: bonus += INFRA_BONUS * g["infra__pharmacy_count_norm"].fillna(0)
    if cb_infra_pc and "infra__pc_cafe_count_norm" in g: bonus += INFRA_BONUS * g["infra__pc_cafe_count_norm"].fillna(0)
    if cb_infra_karaoke and "infra__karaoke_count_norm" in g: bonus += INFRA_BONUS * g["infra__karaoke_count_norm"].fillna(0)
    if cb_infra_fit and "infra__fitness_count_norm" in g: bonus += INFRA_BONUS * g["infra__fitness_count_norm"].fillna(0)
    if cb_infra_yoga and "infra__yoga_pilates_count_norm" in g: bonus += INFRA_BONUS * g["infra__yoga_pilates_count_norm"].fillna(0)
    return bonus

def apply_category_rules_all(g):
    g = g.copy()
    bonus = _compute_bonus_columns(g, selected_category)
    g["NSI"] = (g["NSI_base"] + bonus).clip(0,1)
    return g

def apply_category_rules(g):
    g = g.copy()
    mask = pd.Series(True, index=g.index)
    if selected_category == "ğŸ”¥ í˜„ì¬ ì¸ê¸° ì§€ì—­":
        mask = (g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] >= g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].quantile(0.70))
    elif selected_category == "ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­" and "access_score" in g and g["access_score"].notna().any() and g["access_score"].sum() > 0:
        mask = (g["access_score"] >= g["access_score"].quantile(0.70))
    elif selected_category == "ğŸ› ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­" and "cowork_norm" in g and g["cowork_norm"].notna().any() and g["cowork_norm"].sum() > 0:
        mask = (g["cowork_norm"] >= g["cowork_norm"].quantile(0.70))
    elif selected_category == "ğŸ’° í•©ë¦¬ì ì¸ì¸ ë¹„ìš©" and "cost_index" in g and g["cost_index"].notna().any():
        mask = (g["cost_index"] <= g["cost_index"].quantile(0.30))
    elif selected_category == "ğŸš€ ë¹ ë¥¸ ì¸í„°ë„·" and "internet_mbps" in g and g["internet_mbps"].notna().any():
        mask = (g["internet_mbps"] >= g["internet_mbps"].quantile(0.70))

    g = g.loc[mask].copy()
    bonus = _compute_bonus_columns(g, selected_category)
    g["NSI"] = (g["NSI_base"] + bonus).clip(0,1)
    return g

metrics_all = apply_category_rules_all(metrics_map)
metrics_after_rules = apply_category_rules(metrics_map)

# ----------------------------- ë­í‚¹ ê³„ì‚° -----------------------------
if "selected_region" not in st.session_state:
    st.session_state.selected_region = None
selected_norm = normalize_region_name(st.session_state.selected_region) if st.session_state.selected_region else None

ranked_all = metrics_all.copy()
ranked_all["NSI"]  = ranked_all["NSI"].fillna(ranked_all["NSI_base"]).fillna(0.0).clip(0,1)
ranked_all["rank"] = ranked_all["NSI"].rank(ascending=False, method="min").astype(int)

ranked = metrics_after_rules.copy()
if not ranked.empty:
    ranked["NSI"]  = ranked["NSI"].fillna(ranked["NSI_base"]).fillna(0.0).clip(0,1)
    ranked["rank"] = ranked["NSI"].rank(ascending=False, method="min").astype(int)
else:
    ranked["rank"] = 0

COLOR_TOP1, COLOR_TOP2, COLOR_TOP3 = "#e60049", "#ffd43b", "#4dabf7"
COLOR_SEL, COLOR_BASE = "#51cf66", "#cfd4da"
def pick_color(region_norm, selected_region_norm=None):
    if selected_region_norm and region_norm == selected_region_norm:
        return COLOR_SEL
    if not ranked.empty:
        r_series = ranked.loc[ranked["ì§€ì—­_norm"]==region_norm, "rank"]
        r = int(r_series.min()) if not r_series.empty else 999
        return {1:COLOR_TOP1, 2:COLOR_TOP2, 3:COLOR_TOP3}.get(r, COLOR_BASE)
    return COLOR_BASE

# =============================== ì§€ë„ ===============================
MAP_HEIGHT = 680
with left:
    m = folium.Map(location=[36.5,127.8], zoom_start=7, tiles="cartodbpositron", prefer_canvas=True)
    gj, gj_err = load_geojson_safe(KOREA_GEOJSON)

    if gj:
        # GeoJSONì— ë­í‚¹ ì •ë³´ ì¶”ê°€
        # (ì´í•˜ ì½”ë“œ ë™ì¼)
        pass # The rest of the map rendering code is unchanged

    map_state = st_folium(m, width=None, height=MAP_HEIGHT, key="main_map")
    # (ì´í•˜ ì½”ë“œ ë™ì¼)
    pass

# ============================ ìš°ì¸¡ íŒ¨ë„ ============================
with right:
    # (ì´í•˜ ì½”ë“œ ë™ì¼)
    pass

# ============================ ë­í‚¹/ë‹¤ìš´ë¡œë“œ ============================
st.subheader("ì¶”ì²œ ë­í‚¹")
# (ì´í•˜ ì½”ë“œ ë™ì¼)
pass

# ============================ í‚¤ì›Œë“œ Â· ì¹´í…Œê³ ë¦¬ íƒìƒ‰ ============================
st.markdown("## í‚¤ì›Œë“œ Â· ì¹´í…Œê³ ë¦¬ íƒìƒ‰")
# (ì´í•˜ ì½”ë“œ ë™ì¼)
pass

# ============================ ì¶œì²˜ ============================
st.markdown("""
---
**ë°ì´í„° ì¶œì²˜**
- í•œêµ­ê´€ê´‘ë°ì´í„°ë©: ì§€ì—­ë³„ ë°©ë¬¸ììˆ˜, ì§€ì—­ë³„ ê²€ìƒ‰ê±´ìˆ˜
- ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨: ìƒê°€(ìƒê¶Œ) ì •ë³´
- **í•œêµ­ì² ë„ê³µì‚¬**: KTX ë…¸ì„ ë³„ ì—­ì •ë³´
- í•œêµ­ë¬¸í™”ì •ë³´ì›: ì „êµ­ê³µìœ ì˜¤í”¼ìŠ¤ì‹œì„¤ë°ì´í„°
""")

# (ì „ì²´ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ê¸° ìœ„í•´ ìƒëµëœ ë¶€ë¶„ë“¤ì€ ì´ì „ ì½”ë“œì™€ ë™ì¼í•©ë‹ˆë‹¤.)
# ìœ„ ì½”ë“œì˜ ìƒëµëœ ë¶€ë¶„ë“¤ì„ ì±„ì›Œì„œ ì „ì²´ ì½”ë“œë¥¼ ì™„ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ... [ì´ì „ ë‹µë³€ì˜ ì§€ë„, ìš°ì¸¡íŒ¨ë„, ë­í‚¹, í‚¤ì›Œë“œíƒìƒ‰ ì½”ë“œ ì „ì²´ë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”] ...
# Due to length constraints, the unchanged parts of the UI (map, right panel, ranking table) are omitted here.
# Please paste the corresponding sections from the previous complete code block to finalize the script.
