# -*- coding: utf-8 -*-
import os
import json
import re
import time
import uuid
import streamlit as st
import pandas as pd
import numpy as np

# ì§€ë„
import folium
from folium.features import GeoJson, GeoJsonTooltip, GeoJsonPopup
from streamlit_folium import st_folium

st.set_page_config(page_title="ë””ì§€í„¸ ë…¸ë§ˆë“œ ì§€ì—­ ì¶”ì²œ ëŒ€ì‹œë³´ë“œ", layout="wide")

# ------------------------------- ê²½ë¡œ/í´ë” -------------------------------
APP_DIR  = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(APP_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)
CANDIDATE_BASES = [DATA_DIR, APP_DIR, "/mnt/data"]   # ì§‘ í´ë” ë¯¸ì‚¬ìš©

def build_paths():
    for base in CANDIDATE_BASES:
        fv = os.path.join(base, "20250809144224_ê´‘ì—­ë³„ ë°©ë¬¸ì ìˆ˜.csv")
        fc = os.path.join(base, "PLP_ì—…ì¢…ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv")
        ft = os.path.join(base, "PLP_ìœ í˜•ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv")
        if all(os.path.exists(p) for p in [fv, fc, ft]):
            return fv, fc, ft
    return (
        os.path.join(CANDIDATE_BASES[0], "20250809144224_ê´‘ì—­ë³„ ë°©ë¬¸ì ìˆ˜.csv"),
        os.path.join(CANDIDATE_BASES[0], "PLP_ì—…ì¢…ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv"),
        os.path.join(CANDIDATE_BASES[0], "PLP_ìœ í˜•ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv"),
    )

file_visitors, file_spend_cat, file_spend_type = build_paths()

def resolve_geojson_path():
    env_path = os.environ.get("GEOJSON_PATH", "").strip()
    if env_path and os.path.exists(env_path):
        return env_path
    candidates = [
        os.path.join(DATA_DIR, "korea_provinces.geojson"),
        os.path.join(DATA_DIR, "KOREA_GEOJSON.geojson"),
        os.path.join(APP_DIR,  "korea_provinces.geojson"),
        os.path.join(APP_DIR,  "KOREA_GEOJSON.geojson"),
    ]
    for p in candidates:
        if os.path.exists(p):
            return p
    return ""

KOREA_GEOJSON = resolve_geojson_path()
GEO_PROP_KEYS = ["name", "CTPRVN_NM", "ADM1_KOR_NM", "sido_nm", "SIG_KOR_NM", "NAME_1"]

# ---- ì¸í”„ë¼ í´ë”/ZIP ìë™ íƒìƒ‰(ì§‘ í´ë” X) ----
INFRA_DIR_NAME  = "ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€(ìƒê¶Œ)ì •ë³´_20250630"
INFRA_ZIP_NAME  = "ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€(ìƒê¶Œ)ì •ë³´_20250630.zip"

def resolve_infra_sources():
    # í´ë” ìš°ì„  â†’ ZIP
    for base in CANDIDATE_BASES:
        d = os.path.join(base, INFRA_DIR_NAME)
        if os.path.isdir(d):
            csvs = []
            for r, _, files in os.walk(d):
                for f in files:
                    if f.lower().endswith(".csv"):
                        csvs.append(os.path.join(r, f))
            if csvs:
                return {"mode": "dir", "paths": csvs}
    for base in CANDIDATE_BASES:
        z = os.path.join(base, INFRA_ZIP_NAME)
        if os.path.exists(z):
            return {"mode": "zip", "paths": [z]}
    return {"mode": "none", "paths": []}

# --------------------------- ì•ˆì „ GeoJSON ë¡œë” ---------------------------
@st.cache_data(show_spinner=False)
def load_geojson_safe(path: str):
    if not path or not os.path.exists(path):
        return None, "missing_path"
    try:
        if os.path.getsize(path) == 0:
            return None, "empty_file"
    except Exception:
        pass
    encodings = ("utf-8", "utf-8-sig", "utf-16", "utf-16-le", "utf-16-be", "cp949", "euc-kr", "latin-1")
    last_err = None
    for enc in encodings:
        try:
            with open(path, "r", encoding=enc, errors="strict") as f:
                txt = f.read().strip()
            if not txt:
                return None, f"empty_text({enc})"
            if txt.lstrip().startswith(("var ", "const ", "let ")):
                m = re.search(r"\{.*\}\s*;?\s*$", txt, flags=re.S)
                if m:
                    txt = m.group(0)
            gj = json.loads(txt)
            if not isinstance(gj, dict): return None, f"not_dict({enc})"
            if gj.get("type") == "Topology": return None, "topojson_not_supported"
            if gj.get("type") != "FeatureCollection" or not isinstance(gj.get("features"), list):
                return None, f"not_featurecollection({enc})"
            return gj, None
        except Exception as e:
            last_err = f"{enc}: {e}"
            continue
    return None, last_err or "unknown_error"

# ---------------------------- í‘œì¤€/ì•½ì‹ ì§€ì—­ëª… ----------------------------
# í‘œì‹œìš© 2ê¸€ì ì•½ì‹ëª…ìœ¼ë¡œ í†µì¼ (íˆ´íŒ ì˜ë¦¼ ë°©ì§€)
TWOCHAR_MAP = {
    "ì„œìš¸":"ì„œìš¸","ë¶€ì‚°":"ë¶€ì‚°","ëŒ€êµ¬":"ëŒ€êµ¬","ì¸ì²œ":"ì¸ì²œ","ê´‘ì£¼":"ê´‘ì£¼","ëŒ€ì „":"ëŒ€ì „","ìš¸ì‚°":"ìš¸ì‚°","ì„¸ì¢…":"ì„¸ì¢…",
    "ê²½ê¸°":"ê²½ê¸°","ê°•ì›":"ê°•ì›","ì¶©ë¶":"ì¶©ë¶","ì¶©ë‚¨":"ì¶©ë‚¨","ì „ë¶":"ì „ë¶","ì „ë‚¨":"ì „ë‚¨","ê²½ë¶":"ê²½ë¶","ê²½ë‚¨":"ê²½ë‚¨","ì œì£¼":"ì œì£¼",
}
def to_twochar(s: str) -> str:
    s = str(s)
    if s.startswith("ì „ë¼ë‚¨"): return "ì „ë‚¨"
    if s.startswith("ì „ë¼ë¶"): return "ì „ë¶"
    if s.startswith("ê²½ìƒë‚¨"): return "ê²½ë‚¨"
    if s.startswith("ê²½ìƒë¶"): return "ê²½ë¶"
    if s.startswith("ì¶©ì²­ë‚¨"): return "ì¶©ë‚¨"
    if s.startswith("ì¶©ì²­ë¶"): return "ì¶©ë¶"
    # ê·¸ ì™¸ëŠ” ì ‘ë¯¸ ì œê±° í›„ ë§¤í•‘
    s = re.sub(r"(íŠ¹ë³„ìì¹˜ë„|íŠ¹ë³„ìì¹˜ì‹œ|íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ìì¹˜ë„|ìì¹˜ì‹œ|ë„|ì‹œ)$","", s)
    return TWOCHAR_MAP.get(s, s[:2])

REGION_COORDS = {
    "ì„œìš¸": (37.5665, 126.9780), "ë¶€ì‚°": (35.1796, 129.0756), "ëŒ€êµ¬": (35.8714, 128.6014),
    "ì¸ì²œ": (37.4563, 126.7052), "ê´‘ì£¼": (35.1595, 126.8526), "ëŒ€ì „": (36.3504, 127.3845),
    "ìš¸ì‚°": (35.5384, 129.3114), "ì„¸ì¢…": (36.4800, 127.2890), "ê²½ê¸°": (37.4138, 127.5183),
    "ê°•ì›": (37.8228, 128.1555), "ì¶©ë¶": (36.6357, 127.4913), "ì¶©ë‚¨": (36.5184, 126.8000),
    "ì „ë¶": (35.7175, 127.1530), "ì „ë‚¨": (34.8679, 126.9910), "ê²½ë¶": (36.4919, 128.8889),
    "ê²½ë‚¨": (35.4606, 128.2132), "ì œì£¼": (33.4996, 126.5312),
}

# ----------------------------- CSV ë¡œë” -----------------------------
NEEDED_VIS_COLS = ["ê´‘ì—­ì§€ìì²´ëª…", "ê¸°ì´ˆì§€ìì²´ ë°©ë¬¸ì ìˆ˜"]
NEEDED_PLP_COLS = ["ì§€ì—­", "ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜", "ëŒ€ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨", "ì¤‘ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨"]

@st.cache_data(show_spinner=False)
def read_csv_forgiving(path, usecols=None, dtype=None):
    for enc in ["utf-8", "cp949", "euc-kr"]:
        try:
            return pd.read_csv(path, encoding=enc, usecols=usecols, dtype=dtype)
        except Exception:
            continue
    return pd.read_csv(path, usecols=usecols, dtype=dtype)

@st.cache_data(show_spinner=False)
def read_data():
    vis = read_csv_forgiving(file_visitors, usecols=NEEDED_VIS_COLS, dtype={"ê´‘ì—­ì§€ìì²´ëª…": "string"})
    cat = read_csv_forgiving(file_spend_cat, usecols=NEEDED_PLP_COLS,
                             dtype={"ì§€ì—­": "string", "ëŒ€ë¶„ë¥˜": "string", "ì¤‘ë¶„ë¥˜": "string"})
    typ = read_csv_forgiving(file_spend_type, usecols=NEEDED_PLP_COLS,
                             dtype={"ì§€ì—­": "string", "ëŒ€ë¶„ë¥˜": "string", "ì¤‘ë¶„ë¥˜": "string"})
    for df in (cat, typ):
        for c in ["ì§€ì—­", "ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜"]:
            if c in df.columns:
                df[c] = df[c].astype("category")
    return vis, cat, typ

# ----------------------- ìœ í‹¸/ì •ê·œí™”/ê³„ì‚° -----------------------
def normalize_region_name(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\(.*?\)", "", str(s))
    s = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", s).strip()
    for a in ["íŠ¹ë³„ìì¹˜ë„","íŠ¹ë³„ìì¹˜ì‹œ","íŠ¹ë³„ì‹œ","ê´‘ì—­ì‹œ","ìì¹˜ë„","ìì¹˜ì‹œ","ë„","ì‹œ"]:
        s = s.replace(a, "")
    s = re.sub(r"\s+", " ", s).strip()
    # ê¸´ ì´ë¦„ì„ 2ê¸€ì ì•½ì‹ìœ¼ë¡œ
    s = to_twochar(s)
    return s

def compute_overall_share(df):
    df = df.copy()
    for c in ["ëŒ€ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨", "ì¤‘ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨"]:
        df[c] = pd.to_numeric(df[c], errors="coerce").astype(float)
    df["ëŒ€ë¶„ë¥˜_f"] = df["ëŒ€ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨"].fillna(0)/100.0
    df["ì¤‘ë¶„ë¥˜_f"] = df["ì¤‘ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨"].fillna(0)/100.0
    df["ì¤‘ë¶„ë¥˜_ì „ì²´ë¹„ì¤‘"] = df["ëŒ€ë¶„ë¥˜_f"] * df["ì¤‘ë¶„ë¥˜_f"]
    return df

def minmax(s):
    s = pd.to_numeric(s, errors="coerce")
    if s.notna().sum() == 0:
        return s.fillna(0.0)
    fill_val = s.median(skipna=True)
    s = s.fillna(fill_val)
    d = s.max() - s.min()
    return (s - s.min()) / d if d > 0 else s * 0

BASE_WEIGHTS = dict(vis=0.30, div=0.30, lodg=0.20, act=0.20)

def nsi_with(df):
    return (BASE_WEIGHTS["vis"]  * df["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] +
            BASE_WEIGHTS["div"]  * df["ì†Œë¹„_ë‹¤ì–‘ì„±_norm"] +
            BASE_WEIGHTS["lodg"] * df["ìˆ™ë°•_ë¹„ì¤‘_norm"] +
            BASE_WEIGHTS["act"]  * df["í™œë™_ë‹¤ì–‘ì„±_norm"])

# ----------------------- ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬ -----------------------
try:
    vis, cat, typ = read_data()
except Exception as e:
    st.error("ë°ì´í„° íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n" + str(e))
    st.stop()

# ë°©ë¬¸ì ì§‘ê³„/ì •ê·œí™”
vis_region = (
    vis.groupby("ê´‘ì—­ì§€ìì²´ëª…", as_index=False)["ê¸°ì´ˆì§€ìì²´ ë°©ë¬¸ì ìˆ˜"]
      .sum()
      .rename(columns={"ê¸°ì´ˆì§€ìì²´ ë°©ë¬¸ì ìˆ˜": "ë°©ë¬¸ììˆ˜_í•©ê³„"})
)
total_visitors = max(vis_region["ë°©ë¬¸ììˆ˜_í•©ê³„"].sum(), 1)
vis_region["ë°©ë¬¸ì_ì ìœ ìœ¨"] = vis_region["ë°©ë¬¸ììˆ˜_í•©ê³„"] / total_visitors
vis_region["ì§€ì—­_norm"] = vis_region["ê´‘ì—­ì§€ìì²´ëª…"].map(normalize_region_name)

# ë‹¤ì–‘ì„±/ìˆ™ë°•/í™œë™ ì§€í‘œ
cat2 = compute_overall_share(cat)
typ2 = compute_overall_share(typ)

region_cat = []
for region, g in cat2.groupby("ì§€ì—­"):
    s = g["ì¤‘ë¶„ë¥˜_ì „ì²´ë¹„ì¤‘"].dropna().values
    if len(s) > 0 and s.sum() > 0:
        p = s / s.sum()
        H = -(p * np.log(p)).sum()
        Hmax = np.log(len(p))
        div = H / Hmax if Hmax > 0 else 0.0
    else:
        div = np.nan
    lodg = pd.to_numeric(g.loc[g["ëŒ€ë¶„ë¥˜"] == "ìˆ™ë°•ì—…", "ëŒ€ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨"], errors="coerce")
    lodg_share = float(lodg.mean()) if len(lodg) else np.nan
    region_cat.append({"ì§€ì—­": region, "ì†Œë¹„_ë‹¤ì–‘ì„±ì§€ìˆ˜": div, "ìˆ™ë°•_ì§€ì¶œë¹„ì¤‘(%)": lodg_share})
region_cat = pd.DataFrame(region_cat)
region_cat["ì§€ì—­_norm"] = region_cat["ì§€ì—­"].map(normalize_region_name)

region_typ = []
for region, g in typ2.groupby("ì§€ì—­"):
    s = g["ì¤‘ë¶„ë¥˜_ì „ì²´ë¹„ì¤‘"].dropna().values
    if len(s) > 0 and s.sum() > 0:
        p = s / s.sum()
        H = -(p * np.log(p)).sum()
        Hmax = np.log(len(p))
        div_t = H / Hmax if Hmax > 0 else 0.0
    else:
        div_t = np.nan
    region_typ.append({"ì§€ì—­": region, "í™œë™_ë‹¤ì–‘ì„±ì§€ìˆ˜": div_t})
region_typ = pd.DataFrame(region_typ)
region_typ["ì§€ì—­_norm"] = region_typ["ì§€ì—­"].map(normalize_region_name)

@st.cache_data(show_spinner=False)
def compute_metrics(vis_region, region_cat, region_typ):
    metrics = (
        vis_region.merge(region_cat.drop(columns=["ì§€ì—­"]), on="ì§€ì—­_norm", how="left")
                  .merge(region_typ.drop(columns=["ì§€ì—­"]), on="ì§€ì—­_norm", how="left")
    )
    coords_df = pd.DataFrame([{"ì§€ì—­_norm": k, "lat": v[0], "lon": v[1]} for k, v in REGION_COORDS.items()])
    metrics_map = metrics.copy()
    metrics_map["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] = minmax(metrics_map["ë°©ë¬¸ì_ì ìœ ìœ¨"].fillna(0))
    metrics_map["ì†Œë¹„_ë‹¤ì–‘ì„±_norm"]   = minmax(metrics_map["ì†Œë¹„_ë‹¤ì–‘ì„±ì§€ìˆ˜"].fillna(0))
    metrics_map["ìˆ™ë°•_ë¹„ì¤‘_norm"]     = minmax(metrics_map["ìˆ™ë°•_ì§€ì¶œë¹„ì¤‘(%)"].fillna(0))
    metrics_map["í™œë™_ë‹¤ì–‘ì„±_norm"]   = minmax(metrics_map["í™œë™_ë‹¤ì–‘ì„±ì§€ìˆ˜"].fillna(0))
    metrics_map = metrics_map.merge(coords_df, on="ì§€ì—­_norm", how="left")
    metrics_map["NSI_base"] = nsi_with(metrics_map)
    return metrics_map

metrics_map = compute_metrics(vis_region, region_cat, region_typ)

# ==================== ì¸í”„ë¼ ì§€í‘œ(ìƒê°€ í´ë”/ZIP) í†µí•© ====================
@st.cache_data(show_spinner=True)
def build_infra_from_sources(sources):
    import io, zipfile
    required = {"ì‹œë„ëª…","ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…","ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…","í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…"}
    dfs = []

    if sources["mode"] == "dir":
        for path in sources["paths"]:
            try:
                df = None
                for enc in ["cp949","utf-8","euc-kr","latin1"]:
                    try:
                        df = pd.read_csv(path, encoding=enc, low_memory=False)
                        break
                    except Exception:
                        df = None
                if df is None or not required.issubset(set(df.columns)): 
                    continue
                dfs.append(df[list(required)].copy())
            except Exception:
                continue
    elif sources["mode"] == "zip":
        zpath = sources["paths"][0]
        with zipfile.ZipFile(zpath, "r") as z:
            for name in z.namelist():
                if not name.lower().endswith(".csv"):
                    continue
                raw = z.read(name)
                df = None
                for enc in ["cp949","utf-8","euc-kr","latin1"]:
                    try:
                        df = pd.read_csv(io.BytesIO(raw), encoding=enc, low_memory=False)
                        break
                    except Exception:
                        df = None
                if df is None or not required.issubset(set(df.columns)):
                    continue
                dfs.append(df[list(required)].copy())

    if not dfs:
        return pd.DataFrame()

    df = pd.concat(dfs, ignore_index=True)
    for c in required: df[c] = df[c].astype(str).str.strip()

    mid = df["ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…"].astype(str)
    sub = df["ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…"].astype(str)
    std = df["í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…"].astype(str)

    m_cafe = (sub.str.contains("ì¹´í˜")) | (std.str.contains("ì»¤í”¼ ì „ë¬¸ì "))
    m_conv = (sub.str.contains("í¸ì˜ì ")) | (std.str.contains("ì²´ì¸í™” í¸ì˜ì "))
    m_hotel = sub.str.contains("í˜¸í…”/ë¦¬ì¡°íŠ¸")
    m_motel = sub.str.contains("ì—¬ê´€/ëª¨í…”")
    m_accom_mid = mid.str.contains("ìˆ™ë°•")
    m_pc = sub.str.contains("PCë°©")
    m_laundry = sub.str.contains("ì„¸íƒì†Œ")
    m_pharmacy = sub.str.contains("ì•½êµ­")
    m_clinic = mid.str.contains("ì˜ì›")
    m_hospital = mid.str.contains("ë³‘ì›") | m_clinic | sub.str.contains("ì¹˜ê³¼ì˜ì›|í•œì˜ì›")
    m_library = mid.str.contains("ë„ì„œê´€Â·ì‚¬ì ì§€")

    df["sido_norm"] = df["ì‹œë„ëª…"].map(lambda x: normalize_region_name(x))

    agg = df.groupby("sido_norm").agg(
        total_places=("ì‹œë„ëª…", "size"),
        cafe_count=("ì‹œë„ëª…", lambda s: int(m_cafe.loc[s.index].sum())),
        convenience_count=("ì‹œë„ëª…", lambda s: int(m_conv.loc[s.index].sum())),
        accommodation_count=("ì‹œë„ëª…", lambda s: int((m_hotel|m_motel|m_accom_mid).loc[s.index].sum())),
        hospital_count=("ì‹œë„ëª…", lambda s: int(m_hospital.loc[s.index].sum())),
        pharmacy_count=("ì‹œë„ëª…", lambda s: int(m_pharmacy.loc[s.index].sum())),
        pc_cafe_count=("ì‹œë„ëª…", lambda s: int(m_pc.loc[s.index].sum())),
        laundry_count=("ì‹œë„ëª…", lambda s: int(m_laundry.loc[s.index].sum())),
        library_museum_count=("ì‹œë„ëª…", lambda s: int(m_library.loc[s.index].sum())),
    ).reset_index()

    def per_10k(n, total):
        total = total.replace(0, np.nan)
        return (n / total) * 10000

    for col in [
        "cafe_count","convenience_count","accommodation_count","hospital_count",
        "pharmacy_count","pc_cafe_count","laundry_count","library_museum_count"
    ]:
        agg[col+"_per10k"] = per_10k(agg[col].astype(float), agg["total_places"].astype(float)).round(3)
        v = agg[col].astype(float)
        rng = v.max() - v.min()
        agg[col+"_norm"] = ((v - v.min())/rng).fillna(0).round(4) if rng > 0 else (v*0)
    return agg

infra_sources = resolve_infra_sources()
infra_df = build_infra_from_sources(infra_sources)
if not infra_df.empty:
    metrics_map = metrics_map.merge(
        infra_df.add_prefix("infra__").rename(columns={"infra__sido_norm":"ì§€ì—­_norm"}),
        on="ì§€ì—­_norm", how="left"
    )

# ============================ UI ë ˆì´ì•„ì›ƒ ============================
st.title("ë””ì§€í„¸ ë…¸ë§ˆë“œ ì§€ì—­ ì¶”ì²œ ëŒ€ì‹œë³´ë“œ")
left, right = st.columns([2, 1])
with left:
    st.subheader("ì§€ë„ì—ì„œ ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”")

# -------- ì‚¬ì´ë“œë°”(ì—…ë¡œë” ì œê±°, ê¸°ì¡´ ì»¤ë®¤ë‹ˆí‹°/ì°¨íŠ¸ ìœ ì§€) --------
st.sidebar.header("ì¶”ì²œ ì¹´í…Œê³ ë¦¬ ì„ íƒ")
cb_popular  = st.sidebar.checkbox("ğŸ”¥ í˜„ì¬ ì¸ê¸° ì§€ì—­", value=False)
cb_toprank  = st.sidebar.checkbox("ğŸ… ìƒìœ„ ë­í‚¹ ì§€ì—­", value=True)
cb_hidden   = st.sidebar.checkbox("ğŸ’ ìˆ¨ì€ ë³´ì„ ì§€ì—­", value=False)
cb_act_rich = st.sidebar.checkbox("ğŸ¯ í™œë™ì´ ë‹¤ì–‘í•œ ì§€ì—­", value=False)
cb_lodging  = st.sidebar.checkbox("ğŸ›ï¸ ìˆ™ë°•ì´ ì¢‹ì€ ì§€ì—­", value=False)
cb_diverse  = st.sidebar.checkbox("ğŸ›ï¸ ì†Œë¹„ê°€ ë‹¤ì–‘í•œ ì§€ì—­", value=False)

st.sidebar.markdown("---")
# (í–¥í›„ ì‹¤ì œì§€í‘œ ì—°ê²° ì „ê¹Œì§€ í”Œë ˆì´ìŠ¤í™€ë” ìœ ì§€)
cb_budget   = st.sidebar.checkbox("ğŸ’° ì €ë ´í•œ ë¹„ìš©(í”Œë ˆì´ìŠ¤í™€ë”)", value=False)
cb_fastnet  = st.sidebar.checkbox("ğŸš€ ë¹ ë¥¸ ì¸í„°ë„·(í”Œë ˆì´ìŠ¤í™€ë”)", value=False)
cb_cleanair = st.sidebar.checkbox("ğŸ’¨ ê¹¨ë—í•œ ê³µê¸°(í”Œë ˆì´ìŠ¤í™€ë”)", value=False)
cb_safe     = st.sidebar.checkbox("ğŸ›¡ï¸ ì•ˆì „í•œ ì§€ì—­(í”Œë ˆì´ìŠ¤í™€ë”)", value=False)

st.sidebar.markdown("---")
st.sidebar.caption("ì¸í”„ë¼ ë³´ë„ˆìŠ¤(ìƒê°€ í´ë”/ZIP ê¸°ë°˜)")
cb_infra_cafe   = st.sidebar.checkbox("â˜• ì¹´í˜ ì¸í”„ë¼", value=False)
cb_infra_conv   = st.sidebar.checkbox("ğŸª í¸ì˜ì  ë°€ë„", value=False)
cb_infra_accom  = st.sidebar.checkbox("ğŸ¨ ìˆ™ë°• ì‹œì„¤", value=False)
cb_infra_hosp   = st.sidebar.checkbox("ğŸ¥ ë³‘ì›Â·ì˜ì›", value=False)
cb_infra_pharm  = st.sidebar.checkbox("ğŸ’Š ì•½êµ­", value=False)
cb_infra_pc     = st.sidebar.checkbox("ğŸ–¥ï¸ PCë°©", value=False)
cb_infra_laundry= st.sidebar.checkbox("ğŸ§º ì„¸íƒ ì¸í”„ë¼", value=False)
cb_infra_lib    = st.sidebar.checkbox("ğŸ›ï¸ ë„ì„œê´€Â·ì‚¬ì ì§€", value=False)

st.sidebar.markdown("---")
match_mode      = st.sidebar.radio("í•„í„° ê²°í•© ë°©ì‹", ["ANY(í•˜ë‚˜ ì´ìƒ ì¶©ì¡±)", "ALL(ëª¨ë‘ ì¶©ì¡±)"], index=0)
filter_strength = st.sidebar.slider("í•„í„° ê°•ë„", 0.0, 1.0, 0.5, 0.05)
bonus_strength  = st.sidebar.slider("ë³´ë„ˆìŠ¤ ê°€ì¤‘", 0.0, 0.5, 0.20, 0.01)

# ----------------------------- ë£° ì ìš© -----------------------------
def apply_category_rules(df):
    g0 = df.copy()
    notes = []

    q_vis_hi = g0["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].quantile(0.70)
    q_vis_lo = g0["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].quantile(0.40)
    q_nsi_hi = g0["NSI_base"].quantile(0.70)
    q_div_hi = g0["ì†Œë¹„_ë‹¤ì–‘ì„±_norm"].quantile(0.70)
    q_lod_hi = g0["ìˆ™ë°•_ë¹„ì¤‘_norm"].quantile(0.70)
    q_act_hi = g0["í™œë™_ë‹¤ì–‘ì„±_norm"].quantile(0.70)

    conds = []
    if cb_popular:  conds.append(g0["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] >= (q_vis_hi * (1 - 0.3 * filter_strength))); notes.append("í˜„ì¬ ì¸ê¸°: ë°©ë¬¸ì ìƒìœ„")
    if cb_toprank:  conds.append(g0["NSI_base"]         >= (q_nsi_hi * (1 - 0.3 * filter_strength))); notes.append("ìƒìœ„ ë­í‚¹: NSI ìƒìœ„")
    if cb_hidden:   conds.append((g0["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] <= (q_vis_lo * (1 + 0.3 * filter_strength))) & (g0["NSI_base"] >= q_nsi_hi)); notes.append("ìˆ¨ì€ ë³´ì„: ë°©ë¬¸ì í•˜ìœ„ & NSI ìƒìœ„")
    if cb_act_rich: conds.append(g0["í™œë™_ë‹¤ì–‘ì„±_norm"] >= (q_act_hi * (1 - 0.3 * filter_strength))); notes.append("í™œë™ ë‹¤ì–‘: í™œë™ ë‹¤ì–‘ì„± ìƒìœ„")
    if cb_lodging:  conds.append(g0["ìˆ™ë°•_ë¹„ì¤‘_norm"]   >= (q_lod_hi * (1 - 0.3 * filter_strength))); notes.append("ìˆ™ë°• ì¸í”„ë¼: ìˆ™ë°• ë¹„ì¤‘ ìƒìœ„")
    if cb_diverse:  conds.append(g0["ì†Œë¹„_ë‹¤ì–‘ì„±_norm"] >= (q_div_hi * (1 - 0.3 * filter_strength))); notes.append("ì†Œë¹„ ë‹¤ì–‘: ì†Œë¹„ ë‹¤ì–‘ì„± ìƒìœ„")

    if len(conds) == 0:
        g = g0.copy()
    else:
        if "ALL" in match_mode:
            mask = np.logical_and.reduce(conds)
            g = g0.loc[mask].copy()
            if g.empty:
                mask = np.logical_or.reduce(conds)
                g = g0.loc[mask].copy()
                notes.append("âš ï¸ ALLâ†’ê²°ê³¼ ì—†ìŒ â†’ ANYë¡œ ì™„í™”")
        else:
            mask = np.logical_or.reduce(conds)
            g = g0.loc[mask].copy()
            if g.empty:
                g = g0.copy()
                notes.append("âš ï¸ ê²°ê³¼ ì—†ìŒ â†’ í•„í„° í•´ì œ")

    bonus = np.zeros(len(g))
    def add_bonus(series, strong_q): return bonus_strength * (series - strong_q).clip(lower=0)

    if cb_popular:  bonus += add_bonus(g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"], q_vis_hi)
    if cb_toprank:  bonus += add_bonus(g["NSI_base"], q_nsi_hi)
    if cb_act_rich: bonus += add_bonus(g["í™œë™_ë‹¤ì–‘ì„±_norm"], q_act_hi)
    if cb_lodging:  bonus += add_bonus(g["ìˆ™ë°•_ë¹„ì¤‘_norm"], q_lod_hi)
    if cb_diverse:  bonus += add_bonus(g["ì†Œë¹„_ë‹¤ì–‘ì„±_norm"], q_div_hi)

    def has(col): return col in g.columns and pd.api.types.is_numeric_dtype(g[col])
    infra_cols = {
        "cafe": "infra__cafe_count_norm",
        "conv": "infra__convenience_count_norm",
        "accom": "infra__accommodation_count_norm",
        "hosp": "infra__hospital_count_norm",
        "pharm": "infra__pharmacy_count_norm",
        "pc": "infra__pc_cafe_count_norm",
        "laundry": "infra__laundry_count_norm",
        "lib": "infra__library_museum_count_norm",
    }
    if cb_infra_cafe   and has(infra_cols["cafe"]):     bonus += bonus_strength * g[infra_cols["cafe"]].fillna(0)
    if cb_infra_conv   and has(infra_cols["conv"]):     bonus += bonus_strength * g[infra_cols["conv"]].fillna(0)
    if cb_infra_accom  and has(infra_cols["accom"]):    bonus += bonus_strength * g[infra_cols["accom"]].fillna(0)
    if cb_infra_hosp   and has(infra_cols["hosp"]):     bonus += bonus_strength * g[infra_cols["hosp"]].fillna(0)
    if cb_infra_pharm  and has(infra_cols["pharm"]):    bonus += bonus_strength * g[infra_cols["pharm"]].fillna(0)
    if cb_infra_pc     and has(infra_cols["pc"]):       bonus += bonus_strength * g[infra_cols["pc"]].fillna(0)
    if cb_infra_laundry and has(infra_cols["laundry"]): bonus += bonus_strength * g[infra_cols["laundry"]].fillna(0)
    if cb_infra_lib    and has(infra_cols["lib"]):      bonus += bonus_strength * g[infra_cols["lib"]].fillna(0)

    # ë°ëª¨ìš© placeholder
    if cb_budget and has("cost_index"):
        rng = (g["cost_index"].max() - g["cost_index"].min()) + 1e-9
        tmp = 1 - ((g["cost_index"] - g["cost_index"].min()) / rng)
        bonus += bonus_strength * tmp
    if cb_fastnet and has("internet_mbps"):
        rng = (g["internet_mbps"].max() - g["internet_mbps"].min()) + 1e-9
        tmp = (g["internet_mbps"] - g["internet_mbps"].min()) / rng
        bonus += bonus_strength * tmp
    if cb_cleanair and has("air_quality_pm25"):
        rng = (g["air_quality_pm25"].max() - g["air_quality_pm25"].min()) + 1e-9
        tmp = 1 - ((g["air_quality_pm25"] - g["air_quality_pm25"].min()) / rng)
        bonus += bonus_strength * tmp
    if cb_safe and has("safety_index"):
        rng = (g["safety_index"].max() - g["safety_index"].min()) + 1e-9
        tmp = (g["safety_index"] - g["safety_index"].min()) / rng
        bonus += bonus_strength * tmp

    g["NSI"] = (g["NSI_base"] + bonus).clip(0, 1)
    return g, notes

# ----------------------------- ë­í‚¹ ê³„ì‚° -----------------------------
metrics_after_rules, applied_notes = apply_category_rules(metrics_map)

if "selected_region" not in st.session_state:
    st.session_state.selected_region = None
sel_region    = st.session_state.selected_region
selected_norm = normalize_region_name(sel_region) if sel_region else None

ranked = metrics_after_rules.copy()
ranked["NSI"]  = ranked["NSI"].fillna(ranked["NSI_base"]).fillna(0.0).clip(0,1)
ranked["rank"] = ranked["NSI"].rank(ascending=False, method="min").astype(int)

# ìƒ‰: 1Â·2Â·3ë“± ì „ìš© + ì¼ë°˜
COLOR_TOP1, COLOR_TOP2, COLOR_TOP3 = "#e60049", "#ffd43b", "#4dabf7"
COLOR_SEL, COLOR_BASE = "#51cf66", "#cfd4da"

def pick_color(region_norm, selected_region_norm=None):
    if selected_region_norm and region_norm == selected_region_norm:
        return COLOR_SEL
    r = int(ranked.loc[ranked["ì§€ì—­_norm"] == region_norm, "rank"].min()) if region_norm in ranked["ì§€ì—­_norm"].values else 999
    return {1: COLOR_TOP1, 2: COLOR_TOP2, 3: COLOR_TOP3}.get(r, COLOR_BASE)

# =============================== ì§€ë„(ê´‘ì—­ë§Œ) ===============================
MAP_HEIGHT = 680
with left:
    m = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles="cartodbpositron", prefer_canvas=True)

    gj, gj_err = (None, "no_path")
    if KOREA_GEOJSON and os.path.exists(KOREA_GEOJSON):
        gj, gj_err = load_geojson_safe(KOREA_GEOJSON)

    # ì¢Œí‘œ(ì¤‘ì‹¬ì ) ì¤€ë¹„
    coords_df = pd.DataFrame([{"ì§€ì—­_norm": k, "lat": v[0], "lon": v[1]} for k, v in REGION_COORDS.items()])
    ranked = ranked.drop(columns=[c for c in ["lat","lon"] if c in ranked.columns]) \
                   .merge(coords_df, on="ì§€ì—­_norm", how="left")

    rank_lookup = ranked.set_index("ì§€ì—­_norm")[["rank","NSI"]].to_dict("index")

    if gj is not None:
        # GeoJSON ì†ì„±ì— 2ê¸€ì REGION_NAME + ë­í‚¹/NSI ì£¼ì…
        for ft in gj.get("features", []):
            props = ft.get("properties", {})
            region_raw = None
            for k in GEO_PROP_KEYS:
                if k in props and props[k]:
                    region_raw = props[k]; break
            if region_raw is None:
                textish = [str(v) for v in props.values() if isinstance(v, str)]
                region_raw = max(textish, key=len) if textish else ""
            rname = normalize_region_name(region_raw)      # â† 2ê¸€ì ì•½ì‹ìœ¼ë¡œ í†µì¼
            props["REGION_NAME"] = rname
            stats = rank_lookup.get(rname)
            if stats:
                props["RANK_TXT"] = f"{int(stats['rank'])}ìœ„"
                props["NSI_TXT"]  = f"{float(stats['NSI']):.3f}"
            else:
                props["RANK_TXT"] = "-"
                props["NSI_TXT"]  = "-"
            ft["properties"] = props

        # ì „ì²´ ë©´ì„ ë­í¬ ìƒ‰ìœ¼ë¡œ ì±„ì›€(í…Œë‘ë¦¬ X, ë©´ ì±„ì›€ O)
        def style_function(feature):
            rname = feature["properties"].get("REGION_NAME", "")
            color = pick_color(rname, selected_norm)
            return {
                "fillColor": color, "color": color,
                "weight": 1, "fillOpacity": 0.70, "opacity": 0.9
            }

        def highlight_function(feature):
            return {"fillOpacity": 0.9, "weight": 2}

        GeoJson(
            gj,
            name="regions",
            style_function=style_function,
            highlight_function=highlight_function,
            smooth_factor=1.0,
            tooltip=GeoJsonTooltip(
                fields=["REGION_NAME", "RANK_TXT", "NSI_TXT"],
                aliases=["ì§€ì—­", "ë­í‚¹", "NSI"],
                labels=True, sticky=True,
                style=("background-color: rgba(32,32,32,0.90); color: #fff; "
                       "font-size: 12px; padding: 6px 8px; border-radius: 6px;"
                       "white-space: nowrap;"),  # ì¤„ë°”ê¿ˆ ë°©ì§€
            ),
            popup=GeoJsonPopup(fields=["REGION_NAME"], labels=False),
        ).add_to(m)

        # 1Â·2Â·3ë“± ë²”ë¡€(ë©´ ìƒ‰ ê¸°ì¤€)
        legend_html = f"""
        <div style="
          position: fixed; bottom: 20px; left: 20px; z-index: 9999;
          background: rgba(255,255,255,0.96); padding: 12px 14px; border-radius: 10px;
          box-shadow: 0 2px 10px rgba(0,0,0,0.15); font-size: 13px; color: #222;">
          <div style="font-weight:700; margin-bottom:8px;">ë­í‚¹ ë²”ë¡€</div>
          <div style="display:flex; align-items:center; gap:8px; margin:4px 0;">
            <span style="display:inline-block;width:14px;height:14px;background:{COLOR_TOP1};
                         border:1px solid rgba(0,0,0,.2);border-radius:3px;"></span>
            <span>1ìœ„</span>
          </div>
          <div style="display:flex; align-items:center; gap:8px; margin:4px 0;">
            <span style="display:inline-block;width:14px;height:14px;background:{COLOR_TOP2};
                         border:1px solid rgba(0,0,0,.2);border-radius:3px;"></span>
            <span>2ìœ„</span>
          </div>
          <div style="display:flex; align-items:center; gap:8px; margin:4px 0;">
            <span style="display:inline-block;width:14px;height:14px;background:{COLOR_TOP3};
                         border:1px solid rgba(0,0,0,.2);border-radius:3px;"></span>
            <span>3ìœ„</span>
          </div>
          <div style="display:flex; align-items:center; gap:8px; margin:4px 0;">
            <span style="display:inline-block;width:14px;height:14px;background:{COLOR_BASE};
                         border:1px solid rgba(0,0,0,.2);border-radius:3px;"></span>
            <span>ê·¸ ì™¸</span>
          </div>
        </div>"""
        m.get_root().html.add_child(folium.Element(legend_html))
    else:
        # GeoJSON ì—†ìœ¼ë©´ í¬ì¸íŠ¸ ë§ˆì»¤(ë©´ ì±„ì›€ ë¶ˆê°€)
        for _, r in ranked.iterrows():
            if pd.isna(r.get("lat")) or pd.isna(r.get("lon")):
                continue
            color = pick_color(r["ì§€ì—­_norm"], selected_norm)
            nsi = float(r.get("NSI", r.get("NSI_base", 0.0))) if pd.notna(r.get("NSI", np.nan)) else 0.0
            nsi = max(min(nsi, 1.0), 0.0)
            size = 6 + 14 * nsi
            folium.CircleMarker(
                location=[r["lat"], r["lon"]],
                radius=size, color=color, fill=True, fill_color=color,
                fill_opacity=0.75, opacity=0.9, weight=1,
                popup=r["ì§€ì—­_norm"],
                tooltip=f"{r['ì§€ì—­_norm']} Â· {int(r['rank'])}ìœ„ Â· NSI {nsi:.3f}",
            ).add_to(m)

    # í´ë¦­í•œ ì§€ì—­ ì„¸ì…˜ ë°˜ì˜
    def extract_clicked_name(state):
        if not state: return None
        nm = state.get("last_object_clicked_popup")
        if nm: return normalize_region_name(str(nm))
        obj = state.get("last_active_drawing") or {}
        if isinstance(obj, dict):
            nm = obj.get("properties", {}).get("REGION_NAME")
            if nm: return normalize_region_name(str(nm))
        obj = state.get("last_object_clicked") or {}
        if isinstance(obj, dict):
            nm = obj.get("popup") or (obj.get("properties", {}) or {}).get("REGION_NAME")
            if nm: return normalize_region_name(str(nm))
        return None

    map_state = st_folium(m, width=None, height=MAP_HEIGHT, key="main_map")
    clicked_name = extract_clicked_name(map_state)
    prev_clicked = st.session_state.get("_last_clicked")
    if clicked_name and clicked_name != prev_clicked:
        st.session_state.selected_region = clicked_name
        st.session_state._last_clicked = clicked_name

# ============================ ìš°ì¸¡ íŒ¨ë„(ì»¤ë®¤ë‹ˆí‹° ìœ ì§€) ============================
with right:
    st.subheader("ì»¤ë®¤ë‹ˆí‹°")
    role_col1, role_col2 = st.columns(2)
    with role_col1:
        buddy_on = st.toggle("ğŸ§‘â€ğŸ¤â€ğŸ§‘ ë²„ë”” ì„ íƒ", value=False)
    with role_col2:
        tourist_on = st.toggle("ğŸ§³ ê´€ê´‘ê° ì„ íƒ", value=False)
    st.caption(f"- ë²„ë””: **{'ì°¸ì—¬' if buddy_on else 'ë¯¸ì°¸ì—¬'}**  |  ê´€ê´‘ê°: **{'ì°¸ì—¬' if tourist_on else 'ë¯¸ì°¸ì—¬'}**")

    st.markdown("### ì§€ì—­ í•˜ì´ë¼ì´íŠ¸")
    def region_reasons(row, q):
        msgs = []
        if row["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] >= q["vis_hi"]: msgs.append("ë°©ë¬¸ ìˆ˜ìš”ê°€ ë†’ì•„ìš”")
        if row["ì†Œë¹„_ë‹¤ì–‘ì„±_norm"]   >= q["div_hi"]: msgs.append("ì†Œë¹„ ì¹´í…Œê³ ë¦¬ê°€ ë‹¤ì–‘í•´ìš”")
        if row["ìˆ™ë°•_ë¹„ì¤‘_norm"]     >= q["lod_hi"]: msgs.append("ìˆ™ë°• ì¸í”„ë¼ê°€ ì˜ ê°–ì¶°ì ¸ìš”")
        if row["í™œë™_ë‹¤ì–‘ì„±_norm"]   >= q["act_hi"]: msgs.append("ì²´í—˜Â·í™œë™ ì˜µì…˜ì´ í’ë¶€í•´ìš”")
        if not msgs:
            best = []
            for k, lab in [("ë°©ë¬¸ì_ì ìœ ìœ¨_norm","ë°©ë¬¸ ìˆ˜ìš”"),
                           ("ì†Œë¹„_ë‹¤ì–‘ì„±_norm","ì†Œë¹„ ë‹¤ì–‘ì„±"),
                           ("ìˆ™ë°•_ë¹„ì¤‘_norm","ìˆ™ë°• ì¸í”„ë¼"),
                           ("í™œë™_ë‹¤ì–‘ì„±_norm","í™œë™ ë‹¤ì–‘ì„±")]:
                best.append((row[k], lab))
            best = sorted(best, key=lambda x: x[0], reverse=True)[:2]
            msgs = [f"{lab} ìƒëŒ€ì ìœ¼ë¡œ ìš°ìˆ˜" for _, lab in best]
        return " Â· ".join(msgs)

    q = {
        "vis_hi": ranked["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].quantile(0.70),
        "div_hi": ranked["ì†Œë¹„_ë‹¤ì–‘ì„±_norm"].quantile(0.70),
        "lod_hi": ranked["ìˆ™ë°•_ë¹„ì¤‘_norm"].quantile(0.70),
        "act_hi": ranked["í™œë™_ë‹¤ì–‘ì„±_norm"].quantile(0.70),
    }
    top_show = ranked.sort_values("NSI", ascending=False).head(5)
    for _, r in top_show.iterrows():
        st.write(f"**{r['ì§€ì—­_norm']}** â€” {int(r['rank'])}ìœ„ Â· NSI {float(r['NSI']):.3f}")
        st.caption("Â· " + region_reasons(r, q))

    # QnA Â· ê²Œì‹œíŒ (ê¸°ì¡´ ìœ ì§€)
    st.markdown("### QnA Â· ê²Œì‹œíŒ")
    STORE_PATH = os.path.join(DATA_DIR, "community_qna.json")

    def load_store():
        try:
            with open(STORE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {"posts": []}

    def save_store(data):
        try:
            with open(STORE_PATH, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

    if "qna_store" not in st.session_state:
        st.session_state.qna_store = load_store()
    store = st.session_state.qna_store

    tabs = st.tabs(["ì§ˆë¬¸ ì˜¬ë¦¬ê¸°(QnA)", "ê¸€ì“°ê¸°(ê²Œì‹œíŒ)", "í”¼ë“œ ë³´ê¸°"])
    with tabs[0]:
        with st.form("form_qna"):
            title = st.text_input("ì œëª©", value="")
            content = st.text_area("ë‚´ìš©", height=120, value="")
            region_tag = st.text_input("ê´€ë ¨ ì§€ì—­(ì„ íƒ, ì˜ˆ: ì œì£¼Â·ê°•ì›)", value=st.session_state.selected_region or "")
            submit = st.form_submit_button("ì§ˆë¬¸ ë“±ë¡")
        if submit and title.strip():
            store["posts"].append({
                "id": str(uuid.uuid4()), "type": "qna",
                "title": title.strip(), "content": content.strip(),
                "region": normalize_region_name(region_tag) if region_tag else "",
                "author": "ìµëª…", "created": int(time.time()), "answers": []
            })
            save_store(store); st.success("ì§ˆë¬¸ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    with tabs[1]:
        with st.form("form_board"):
            title2 = st.text_input("ì œëª© ", value="")
            content2 = st.text_area("ë³¸ë¬¸", height=140, value="")
            region_tag2 = st.text_input("ì§€ì—­ íƒœê·¸(ì„ íƒ)", value=st.session_state.selected_region or "")
            submit2 = st.form_submit_button("ê¸€ ë“±ë¡")
        if submit2 and title2.strip():
            store["posts"].append({
                "id": str(uuid.uuid4()), "type": "board",
                "title": title2.strip(), "content": content2.strip(),
                "region": normalize_region_name(region_tag2) if region_tag2 else "",
                "author": "ìµëª…", "created": int(time.time()), "comments": []
            })
            save_store(store); st.success("ê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    with tabs[2]:
        colf1, colf2 = st.columns([1,1])
        with colf1:
            feed_type = st.multiselect("ìœ í˜•", ["qna", "board"], default=["qna", "board"])
        with colf2:
            feed_region = st.text_input("ì§€ì—­ í•„í„°(ë¶€ë¶„ì¼ì¹˜)", value="")
        posts = [p for p in store["posts"] if p["type"] in feed_type]
        if feed_region.strip():
            key = normalize_region_name(feed_region)
            posts = [p for p in posts if key in normalize_region_name(p.get("region",""))]
        posts = sorted(posts, key=lambda p: p.get("created", 0), reverse=True)
        for p in posts:
            with st.expander(f"[{'QnA' if p['type']=='qna' else 'ê²Œì‹œê¸€'}] {p['title']}  Â·  {p.get('region','') or 'ì „ì²´'}"):
                st.write(p["content"] or "(ë‚´ìš© ì—†ìŒ)")
                if p["type"] == "qna":
                    for a in p.get("answers", []):
                        st.markdown(f"- **ë‹µë³€**: {a['content']}  â€” _{a.get('author','ìµëª…')}_")
                    with st.form(f"ans_{p['id']}"):
                        ans = st.text_input("ë‹µë³€ ë‹¬ê¸°", value="")
                        ans_btn = st.form_submit_button("ë“±ë¡")
                    if ans_btn and ans.strip():
                        p.setdefault("answers", []).append({"content": ans.strip(), "author": "ìµëª…", "created": int(time.time())})
                        save_store(store); st.success("ë‹µë³€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    for cmt in p.get("comments", []):
                        st.markdown(f"- **ëŒ“ê¸€**: {cmt['content']}  â€” _{cmt.get('author','ìµëª…')}_")
                    with st.form(f"cmt_{p['id']}"):
                        cmt = st.text_input("ëŒ“ê¸€ ë‹¬ê¸°", value="")
                        cmt_btn = st.form_submit_button("ë“±ë¡")
                    if cmt_btn and cmt.strip():
                        p.setdefault("comments", []).append({"content": cmt.strip(), "author": "ìµëª…", "created": int(time.time())})
                        save_store(store); st.success("ëŒ“ê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ============================ ë­í‚¹/í‚¤ì›Œë“œ ============================
st.subheader("ì¶”ì²œ ë­í‚¹")
cols_to_show = ["ê´‘ì—­ì§€ìì²´ëª…","NSI","NSI_base","ë°©ë¬¸ììˆ˜_í•©ê³„","ë°©ë¬¸ì_ì ìœ ìœ¨",
     "ìˆ™ë°•_ì§€ì¶œë¹„ì¤‘(%)","ì†Œë¹„_ë‹¤ì–‘ì„±ì§€ìˆ˜","í™œë™_ë‹¤ì–‘ì„±ì§€ìˆ˜"]
if not infra_df.empty:
    cols_to_show += [
        "infra__cafe_count_per10k","infra__convenience_count_per10k","infra__accommodation_count_per10k",
        "infra__hospital_count_per10k","infra__pharmacy_count_per10k"
    ]
rec = ranked.sort_values("NSI", ascending=False)[cols_to_show]

out = rec.copy()
if "ë°©ë¬¸ììˆ˜_í•©ê³„" in out.columns:
    out["ë°©ë¬¸ììˆ˜_í•©ê³„"] = out["ë°©ë¬¸ììˆ˜_í•©ê³„"].fillna(0).astype(int)
for c in out.columns:
    if c not in ["ê´‘ì—­ì§€ìì²´ëª…"]:
        out[c] = pd.to_numeric(out[c], errors="coerce").round(4)

st.dataframe(rec.reset_index(drop=True), use_container_width=True)
st.download_button(
    "â¬‡ï¸ ë­í‚¹ CSV ì €ì¥",
    out.to_csv(index=False).encode("utf-8-sig"),
    file_name="ranking_by_categories_and_infra.csv", mime="text/csv"
)

st.subheader("í‚¤ì›Œë“œ Â· ì¹´í…Œê³ ë¦¬ íƒìƒ‰ (ì—…ì¢…/ìœ í˜• ê²€ìƒ‰ë¹„ì¤‘ ê¸°ë°˜)")
def safe_categories(series):
    try:
        if hasattr(series, "cat"):
            vals = list(series.cat.categories)
        else:
            vals = pd.Series(series, dtype="string").dropna().unique().tolist()
        vals = [v for v in map(str, vals) if v.strip()]
    except Exception:
        vals = []
    return ["--ì „ì²´--"] + sorted(vals)

col1, col2, col3 = st.columns(3)
with col1:
    st.text_input("ì§€ì—­", st.session_state.selected_region or "", disabled=True)
with col2:
    sel_big = st.selectbox("ëŒ€ë¶„ë¥˜ ì„ íƒ(ì—…ì¢…)", safe_categories(cat["ëŒ€ë¶„ë¥˜"]))
with col3:
    kw = st.text_input("í‚¤ì›Œë“œ(ì¤‘ë¶„ë¥˜ëª… ë¶€ë¶„ ì¼ì¹˜)", "")

def top_keywords(df, region, big=None, keyword="", topn=10):
    region_normed = normalize_region_name(region or "")
    g = df[df["ì§€ì—­"].astype(str).map(normalize_region_name) == region_normed].copy()
    if big and big != "--ì „ì²´--":
        g = g[g["ëŒ€ë¶„ë¥˜"] == big]
    if keyword:
        g = g[g["ì¤‘ë¶„ë¥˜"].astype(str).str.contains(keyword, case=False, na=False)]
    g["ì¤‘ë¶„ë¥˜_ì „ì²´ë¹„ì¤‘"] = (
        pd.to_numeric(g["ëŒ€ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨"], errors="coerce")/100.0 *
        pd.to_numeric(g["ì¤‘ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨"], errors="coerce")/100.0
    )
    g = g.groupby(["ëŒ€ë¶„ë¥˜","ì¤‘ë¶„ë¥˜"], as_index=False)["ì¤‘ë¶„ë¥˜_ì „ì²´ë¹„ì¤‘"].sum() \
         .sort_values("ì¤‘ë¶„ë¥˜_ì „ì²´ë¹„ì¤‘", ascending=False).head(topn)
    return g

if st.session_state.selected_region:
    tabs2 = st.tabs(["ì—…ì¢… ê¸°ì¤€(PLP_ì—…ì¢…ë³„)", "ìœ í˜• ê¸°ì¤€(PLP_ìœ í˜•ë³„)"])
    with tabs2[0]:
        top_cat = top_keywords(cat, st.session_state.selected_region, sel_big, kw, topn=12)
        st.dataframe(top_cat, use_container_width=True)
        st.bar_chart(top_cat.set_index("ì¤‘ë¶„ë¥˜")["ì¤‘ë¶„ë¥˜_ì „ì²´ë¹„ì¤‘"])
    with tabs2[1]:
        sel_big2 = st.selectbox("ëŒ€ë¶„ë¥˜ ì„ íƒ(ìœ í˜•)", safe_categories(typ["ëŒ€ë¶„ë¥˜"]), key="type_big")
        kw2 = st.text_input("í‚¤ì›Œë“œ(ì¤‘ë¶„ë¥˜ëª… ë¶€ë¶„ ì¼ì¹˜)", "", key="type_kw")
        top_typ = top_keywords(typ, st.session_state.selected_region, sel_big2, kw2, topn=12)
        st.dataframe(top_typ, use_container_width=True)
        st.bar_chart(top_typ.set_index("ì¤‘ë¶„ë¥˜")["ì¤‘ë¶„ë¥˜_ì „ì²´ë¹„ì¤‘"])
else:
    st.info("ì§€ë„ì—ì„œ ì˜ì—­ì„ í´ë¦­í•˜ê±°ë‚˜, ìš°ì¸¡ íŒ¨ë„Â·ë“œë¡­ë‹¤ìš´ì„ ì´ìš©í•´ ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”.")

st.markdown("""
---
**ë°ì´í„° ì¶œì²˜**  
- ê´€ê´‘ ë°ì´í„°ë©: ê´‘ì—­ë³„ ë°©ë¬¸ì ìˆ˜  
- PLP ë°ì´í„°: ì—…ì¢…/ìœ í˜•ë³„ ì§€ì¶œë¹„ì¤‘(ê²€ìƒ‰ ë¹„ì¤‘ ê¸°ë°˜)  
- ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨: ìƒê°€(ìƒê¶Œ)ì •ë³´ (í´ë”/ZIP, ì‹œë„ ì¸í”„ë¼ ì§‘ê³„)
""")
