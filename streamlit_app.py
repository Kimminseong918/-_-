# -*- coding: utf-8 -*-
# streamlit run streamlit_app.py
import os, json, re, time, uuid
import streamlit as st
import pandas as pd
import numpy as np

# ì§€ë„
import folium
from folium.features import GeoJson, GeoJsonTooltip, GeoJsonPopup
from streamlit_folium import st_folium

# ========================= ê¸°ë³¸ ì„¤ì • =========================
st.set_page_config(page_title="ë””ì§€í„¸ ë…¸ë§ˆë“œ ì§€ì—­ ì¶”ì²œ ëŒ€ì‹œë³´ë“œ", layout="wide")

APP_DIR  = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(APP_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)
CANDIDATE_BASES = [DATA_DIR, APP_DIR, "/mnt/data"]

# ------------------------------- íŒŒì¼ ê²½ë¡œ -------------------------------
def build_paths():
    for base in CANDIDATE_BASES:
        fv = os.path.join(base, "20250809144224_ê´‘ì—­ë³„ ë°©ë¬¸ì ìˆ˜.csv")
        fc = os.path.join(base, "PLP_ì—…ì¢…ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv")
        ft = os.path.join(base, "PLP_ìœ í˜•ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv")
        if all(os.path.exists(p) for p in [fv, fc, ft]):
            return fv, fc, ft
    return (
        os.path.join(CANDIDATE_BASES[0], "20250809144224_ê´‘ì—­ë³„ ë°©ë¬¸ì ìˆ˜.csv"),
        os.path.join(CANDIDATE_BASES[0], "PLP_ì—…ì¢…ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv"),
        os.path.join(CANDIDATE_BASES[0], "PLP_ìœ í˜•ë³„_ê²€ìƒ‰ê±´ìˆ˜_í†µí•©.csv"),
    )

file_visitors, file_search_cat, file_search_type = build_paths()

def find_optional_file(names):
    for base in CANDIDATE_BASES:
        for n in names:
            p=os.path.join(base,n)
            if os.path.exists(p): return p
    return ""

# GeoJSON(ì„ íƒ)
def resolve_geojson_path():
    env_path = os.environ.get("GEOJSON_PATH", "").strip()
    if env_path and os.path.exists(env_path):
        return env_path
    for p in [
        os.path.join(DATA_DIR, "korea_provinces.geojson"),
        os.path.join(DATA_DIR, "KOREA_GEOJSON.geojson"),
        os.path.join(APP_DIR,  "korea_provinces.geojson"),
        os.path.join(APP_DIR,  "KOREA_GEOJSON.geojson"),
    ]:
        if os.path.exists(p):
            return p
    return ""

KOREA_GEOJSON = resolve_geojson_path()
GEO_PROP_KEYS = ["name", "CTPRVN_NM", "ADM1_KOR_NM", "sido_nm", "SIG_KOR_NM", "NAME_1"]

# ---- ì¸í”„ë¼ í´ë”/ZIP/ë‹¨ì¼ CSV ìë™ íƒìƒ‰ ----
INFRA_DIR_NAME  = "ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€(ìƒê¶Œ)ì •ë³´_20250630"
INFRA_ZIP_NAME  = "ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€(ìƒê¶Œ)ì •ë³´_20250630.zip"

def resolve_infra_sources():
    # í´ë” â†’ ZIP â†’ ë‹¨ì¼ CSV(ì˜ˆ: ê°•ì›ë§Œ) ìˆœ
    for base in CANDIDATE_BASES:
        d = os.path.join(base, INFRA_DIR_NAME)
        if os.path.isdir(d):
            csvs = []
            for r, _, files in os.walk(d):
                for f in files:
                    if f.lower().endswith(".csv"):
                        csvs.append(os.path.join(r, f))
            if csvs:
                return {"mode": "dir", "paths": csvs}
    for base in CANDIDATE_BASES:
        z = os.path.join(base, INFRA_ZIP_NAME)
        if os.path.exists(z):
            return {"mode": "zip", "paths": [z]}
    singles = []
    for base in CANDIDATE_BASES:
        if not os.path.isdir(base): continue
        for f in os.listdir(base):
            if f.lower().endswith(".csv") and ("ìƒê°€" in f or "ì†Œìƒê³µì¸" in f):
                singles.append(os.path.join(base, f))
    if singles:
        return {"mode": "single", "paths": singles}
    return {"mode": "none", "paths": []}

# ---------------------------- ì§€ì—­ëª… ì •ê·œí™” ----------------------------
TWOCHAR_MAP = {"ì„œìš¸":"ì„œìš¸","ë¶€ì‚°":"ë¶€ì‚°","ëŒ€êµ¬":"ëŒ€êµ¬","ì¸ì²œ":"ì¸ì²œ","ê´‘ì£¼":"ê´‘ì£¼","ëŒ€ì „":"ëŒ€ì „","ìš¸ì‚°":"ìš¸ì‚°","ì„¸ì¢…":"ì„¸ì¢…",
               "ê²½ê¸°":"ê²½ê¸°","ê°•ì›":"ê°•ì›","ì¶©ë¶":"ì¶©ë¶","ì¶©ë‚¨":"ì¶©ë‚¨","ì „ë¶":"ì „ë¶","ì „ë‚¨":"ì „ë‚¨","ê²½ë¶":"ê²½ë¶","ê²½ë‚¨":"ê²½ë‚¨","ì œì£¼":"ì œì£¼"}

def to_twochar(s: str) -> str:
    s = str(s)
    if s.startswith("ì „ë¼ë‚¨"): return "ì „ë‚¨"
    if s.startswith("ì „ë¼ë¶"): return "ì „ë¶"
    if s.startswith("ê²½ìƒë‚¨"): return "ê²½ë‚¨"
    if s.startswith("ê²½ìƒë¶"): return "ê²½ë¶"
    if s.startswith("ì¶©ì²­ë‚¨"): return "ì¶©ë‚¨"
    if s.startswith("ì¶©ì²­ë¶"): return "ì¶©ë¶"
    s = re.sub(r"(íŠ¹ë³„ìì¹˜ë„|íŠ¹ë³„ìì¹˜ì‹œ|íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ìì¹˜ë„|ìì¹˜ì‹œ|ë„|ì‹œ)$","", s)
    return TWOCHAR_MAP.get(s, s[:2])

REGION_COORDS = {
    "ì„œìš¸": (37.5665, 126.9780), "ë¶€ì‚°": (35.1796, 129.0756), "ëŒ€êµ¬": (35.8714, 128.6014),
    "ì¸ì²œ": (37.4563, 126.7052), "ê´‘ì£¼": (35.1595, 126.8526), "ëŒ€ì „": (36.3504, 127.3845),
    "ìš¸ì‚°": (35.5384, 129.3114), "ì„¸ì¢…": (36.4800, 127.2890), "ê²½ê¸°": (37.4138, 127.5183),
    "ê°•ì›": (37.8228, 128.1555), "ì¶©ë¶": (36.6357, 127.4913), "ì¶©ë‚¨": (36.5184, 126.8000),
    "ì „ë¶": (35.7175, 127.1530), "ì „ë‚¨": (34.8679, 126.9910), "ê²½ë¶": (36.4919, 128.8889),
    "ê²½ë‚¨": (35.4606, 128.2132), "ì œì£¼": (33.4996, 126.5312),
}

def normalize_region_name(s: str) -> str:
    if not s: return ""
    s = re.sub(r"\(.*?\)", "", str(s))
    s = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", s).strip()
    for a in ["íŠ¹ë³„ìì¹˜ë„","íŠ¹ë³„ìì¹˜ì‹œ","íŠ¹ë³„ì‹œ","ê´‘ì—­ì‹œ","ìì¹˜ë„","ìì¹˜ì‹œ","ë„","ì‹œ"]:
        s = s.replace(a, "")
    s = re.sub(r"\s+", " ", s).strip()
    return to_twochar(s)

def minmax(s):
    s = pd.to_numeric(s, errors="coerce")
    if s.notna().sum() == 0: return s.fillna(0.0)
    d = s.max() - s.min()
    return (s - s.min())/d if d>0 else s*0

# ----------------------------- ë¡œë”/ìºì‹œ -----------------------------
@st.cache_data(show_spinner=False)
def load_geojson_safe(path: str):
    if not path or not os.path.exists(path): return None, "missing_path"
    try:
        if os.path.getsize(path) == 0: return None, "empty_file"
    except Exception:
        pass
    for enc in ("utf-8","utf-8-sig","utf-16","utf-16-le","utf-16-be","cp949","euc-kr","latin-1"):
        try:
            with open(path,"r",encoding=enc,errors="strict") as f:
                txt=f.read().strip()
            if not txt: return None, f"empty_text({enc})"
            if txt.lstrip().startswith(("var ","const ","let ")):
                m=re.search(r"\{.*\}\s*;?\s*$", txt, flags=re.S)
                if m: txt=m.group(0)
            gj=json.loads(txt)
            if not isinstance(gj,dict): continue
            if gj.get("type")=="Topology": return None, "topojson_not_supported"
            if gj.get("type")!="FeatureCollection" or not isinstance(gj.get("features"),list):
                continue
            return gj, None
        except Exception:
            continue
    return None, "parse_error"

@st.cache_data(show_spinner=False)
def read_csv_forgiving(path, usecols=None, dtype=None):
    for enc in ["utf-8","cp949","euc-kr","utf-8-sig","latin1"]:
        try:
            return pd.read_csv(path, encoding=enc, usecols=usecols, dtype=dtype, low_memory=False)
        except Exception:
            continue
    return pd.read_csv(path, usecols=usecols, dtype=dtype, low_memory=False)

# ----- ë°©ë¬¸ì CSV ìë™ ì»¬ëŸ¼ ê°ì§€ -----
VIS_REGION_KEYS = ["ê´‘ì—­ì§€ìì²´ëª…","ê´‘ì—­ì‹œë„","ì‹œë„","ì‹œë„ëª…","region","sido","province"]
VIS_COUNT_KEYS  = ["ê¸°ì´ˆì§€ìì²´ ë°©ë¬¸ì ìˆ˜","ë°©ë¬¸ììˆ˜","ë°©ë¬¸ì ìˆ˜","í•©ê³„","total","count"]

@st.cache_data(show_spinner=False)
def read_visitors_flexible(path):
    if not os.path.exists(path): return pd.DataFrame(), (None, None)
    df = read_csv_forgiving(path)
    if df is None or df.empty: return pd.DataFrame(), (None, None)
    cols = {c.lower(): c for c in df.columns}

    def pick(cands):
        for k in cands:
            if k in df.columns: return k
            if k.lower() in cols: return cols[k.lower()]
        return None

    region_col = pick(VIS_REGION_KEYS)
    count_col  = pick(VIS_COUNT_KEYS)

    if not region_col or not count_col:
        string_cols = [c for c in df.columns if df[c].dtype==object]
        num_cols    = [c for c in df.columns if pd.to_numeric(df[c], errors="coerce").notna().any()]
        region_col = region_col or (string_cols[0] if string_cols else None)
        count_col  = count_col  or (num_cols[0] if num_cols else None)
        if not region_col or not count_col:
            return pd.DataFrame(), (None, None)

    df = df[[region_col, count_col]].copy()
    df[region_col] = df[region_col].astype("string")
    df[count_col]  = pd.to_numeric(df[count_col], errors="coerce")
    return df, (region_col, count_col)

# ----- ê²€ìƒ‰ê±´ìˆ˜ íŒŒì¼ ë¡œë”: ìœ ì—° ì»¬ëŸ¼ ê°ì§€ + Fallback -----
@st.cache_data(show_spinner=False)
def load_search_counts(path):
    if not path or not os.path.exists(path):
        return pd.DataFrame(), (None, None, None)
    df=None
    for enc in ["utf-8","cp949","euc-kr","utf-8-sig","latin1"]:
        try:
            df=pd.read_csv(path, encoding=enc, low_memory=False); break
        except Exception:
            df=None
    if df is None or df.empty: return pd.DataFrame(), (None, None, None)

    cols={c.lower():c for c in df.columns}
    rcol=None
    for k in ["ì§€ì—­","ì‹œë„","ì‹œë„ëª…","ê´‘ì—­ì§€ìì²´","sido","region","province"]:
        if k in cols: rcol=cols[k]; break
    gcol=None
    for k in ["ì¤‘ë¶„ë¥˜","ëŒ€ë¶„ë¥˜","ì—…ì¢…","ì¹´í…Œê³ ë¦¬","ìœ í˜•","í‚¤ì›Œë“œ","ê²€ìƒ‰ì–´","í•­ëª©"]:
        if k in cols: gcol=cols[k]; break
    vcol=None
    for k in ["ê²€ìƒ‰ê±´ìˆ˜","ê²€ìƒ‰ ê±´ìˆ˜","ê²€ìƒ‰ìˆ˜","ê²€ìƒ‰ëŸ‰","count","ê±´ìˆ˜","value","í•©ê³„","ì´ê±´ìˆ˜"]:
        if k in cols: vcol=cols[k]; break

    if not (rcol and gcol and vcol):
        string_cols = [c for c in df.columns if df[c].dtype==object]
        num_cols    = [c for c in df.columns if pd.to_numeric(df[c], errors="coerce").notna().any()]
        if len(string_cols)>=2 and len(num_cols)>=1:
            rcol = rcol or string_cols[0]
            gcol = gcol or string_cols[1]
            vcol = vcol or num_cols[0]
        else:
            return pd.DataFrame(), (None, None, None)

    df[vcol]=pd.to_numeric(df[vcol], errors="coerce").fillna(0)
    return df, (rcol,gcol,vcol)

# ======================== ë°ì´í„° ë¡œë”©/ì „ì²˜ë¦¬ ========================
# ë°©ë¬¸ì
vis_df_raw, vis_cols = read_visitors_flexible(file_visitors)
if vis_df_raw.empty:
    st.error("ë°©ë¬¸ì ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼/ì»¬ëŸ¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()
_region_col, _count_col = vis_cols
vis = (vis_df_raw.groupby(_region_col, as_index=False)[_count_col].sum()
       .rename(columns={_region_col:"ê´‘ì—­ì§€ìì²´ëª…", _count_col:"ë°©ë¬¸ììˆ˜_í•©ê³„"}))

vis_region = vis.copy()
total_visitors = max(vis_region["ë°©ë¬¸ììˆ˜_í•©ê³„"].sum(), 1)
vis_region["ë°©ë¬¸ì_ì ìœ ìœ¨"] = vis_region["ë°©ë¬¸ììˆ˜_í•©ê³„"] / total_visitors
vis_region["ì§€ì—­_norm"] = vis_region["ê´‘ì—­ì§€ìì²´ëª…"].map(normalize_region_name)

metrics_map = vis_region.copy()
coords_df = pd.DataFrame([{"ì§€ì—­_norm":k,"lat":v[0],"lon":v[1]} for k,v in REGION_COORDS.items()])
metrics_map = metrics_map.merge(coords_df, on="ì§€ì—­_norm", how="left")
metrics_map["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] = minmax(metrics_map["ë°©ë¬¸ì_ì ìœ ìœ¨"].fillna(0))

# ìˆ™ë°• ë¹„ì¤‘(ì—†ìœ¼ë©´ 0 ì²˜ë¦¬)
if "ìˆ™ë°•_ì§€ì¶œë¹„ì¤‘(%)" not in metrics_map:
    metrics_map["ìˆ™ë°•_ì§€ì¶œë¹„ì¤‘(%)"] = np.nan
metrics_map["ìˆ™ë°•_ë¹„ì¤‘_norm"] = minmax(metrics_map["ìˆ™ë°•_ì§€ì¶œë¹„ì¤‘(%)"].fillna(0))
metrics_map["NSI_base"] = 0.60*metrics_map["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] + 0.40*metrics_map["ìˆ™ë°•_ë¹„ì¤‘_norm"]

# ==================== ì¸í”„ë¼ ì§€í‘œ(ìƒê°€ í´ë”/ZIP/ë‹¨ì¼ CSV) ====================
@st.cache_data(show_spinner=True)
def build_infra_from_sources(sources):
    import io, zipfile
    required = {"ì‹œë„ëª…","ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…","ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…","í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…"}
    dfs = []

    def try_read_csv(path):
        df=None
        for enc in ["cp949","utf-8","euc-kr","latin1"]:
            try:
                df=pd.read_csv(path, encoding=enc, low_memory=False); break
            except Exception:
                df=None
        return df

    if sources["mode"] == "dir":
        for path in sources["paths"]:
            df = try_read_csv(path)
            if df is None or not required.issubset(set(df.columns)): 
                continue
            dfs.append(df[list(required)].copy())

    elif sources["mode"] == "zip":
        zpath = sources["paths"][0]
        with zipfile.ZipFile(zpath,"r") as z:
            for name in z.namelist():
                if not name.lower().endswith(".csv"): continue
                raw=z.read(name); df=None
                for enc in ["cp949","utf-8","euc-kr","latin1"]:
                    try:
                        df=pd.read_csv(io.BytesIO(raw), encoding=enc, low_memory=False); break
                    except Exception:
                        df=None
                if df is None or not required.issubset(set(df.columns)): 
                    continue
                dfs.append(df[list(required)].copy())

    elif sources["mode"] == "single":
        for path in sources["paths"]:
            df = try_read_csv(path)
            if df is None or not required.issubset(set(df.columns)):
                continue
            dfs.append(df[list(required)].copy())

    if not dfs: 
        return pd.DataFrame()

    df=pd.concat(dfs, ignore_index=True)
    for c in required: df[c]=df[c].astype(str).str.strip()

    mid=df["ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…"].astype(str)
    sub=df["ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…"].astype(str)
    std=df["í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…"].astype(str)
    m_cafe=(sub.str.contains("ì¹´í˜")) | (std.str.contains("ì»¤í”¼ ì „ë¬¸ì "))
    m_conv=(sub.str.contains("í¸ì˜ì ")) | (std.str.contains("ì²´ì¸í™” í¸ì˜ì "))
    m_hotel=sub.str.contains("í˜¸í…”/ë¦¬ì¡°íŠ¸"); m_motel=sub.str.contains("ì—¬ê´€/ëª¨í…”"); m_accom_mid=mid.str.contains("ìˆ™ë°•")
    m_pc=sub.str.contains("PCë°©"); m_laundry=sub.str.contains("ì„¸íƒì†Œ|ë¹¨ë˜ë°©")
    m_pharm=sub.str.contains("ì•½êµ­")
    m_clinic=mid.str.contains("ì˜ì›")
    m_hospital=mid.str.contains("ë³‘ì›") | m_clinic | sub.str.contains("ì¹˜ê³¼ì˜ì›|í•œì˜ì›|ë‚´ê³¼|ì™¸ê³¼|í”¼ë¶€|ë¹„ë‡¨")
    m_library=mid.str.contains("ë„ì„œê´€Â·ì‚¬ì ì§€|ë„ì„œê´€")

    df["sido_norm"]=df["ì‹œë„ëª…"].map(normalize_region_name)
    agg=df.groupby("sido_norm").agg(
        total_places=("ì‹œë„ëª…","size"),
        cafe_count=("ì‹œë„ëª…", lambda s:int(m_cafe.loc[s.index].sum())),
        convenience_count=("ì‹œë„ëª…", lambda s:int(m_conv.loc[s.index].sum())),
        accommodation_count=("ì‹œë„ëª…", lambda s:int((m_hotel|m_motel|m_accom_mid).loc[s.index].sum())),
        hospital_count=("ì‹œë„ëª…", lambda s:int(m_hospital.loc[s.index].sum())),
        pharmacy_count=("ì‹œë„ëª…", lambda s:int(m_pharmacy.loc[s.index].sum())),
        pc_cafe_count=("ì‹œë„ëª…", lambda s:int(m_pc.loc[s.index].sum())),
        laundry_count=("ì‹œë„ëª…", lambda s:int(m_laundry.loc[s.index].sum())),
        library_museum_count=("ì‹œë„ëª…", lambda s:int(m_library.loc[s.index].sum())),
    ).reset_index()

    def per_10k(n,total): total=total.replace(0,np.nan); return (n/total)*10000
    for col in ["cafe_count","convenience_count","accommodation_count","hospital_count",
                "pharmacy_count","pc_cafe_count","laundry_count","library_museum_count"]:
        agg[col+"_per10k"] = per_10k(agg[col].astype(float), agg["total_places"].astype(float)).round(3)
        v=agg[col].astype(float); rng=v.max()-v.min()
        agg[col+"_norm"] = ((v-v.min())/rng).fillna(0).round(4) if rng>0 else v*0
    return agg

# ==================== êµí†µ ì ‘ê·¼ì„± + KTX ====================
@st.cache_data(show_spinner=False)
def load_transport(path):
    if not path: return pd.DataFrame()
    df=None
    for enc in ["utf-8","cp949","euc-kr","utf-8-sig","latin1"]:
        try: df=pd.read_csv(path, encoding=enc); break
        except Exception: df=None
    if df is None: return pd.DataFrame()
    cols={c.lower():c for c in df.columns}
    sidocol=cols.get("sido") or cols.get("ì‹œë„") or cols.get("ì‹œë„ëª…")
    if not sidocol: return pd.DataFrame()
    df["_sido_"]=df[sidocol].astype(str).map(normalize_region_name)
    def pick(*names):
        for n in names:
            if n in df.columns: return n
        for n in names:
            if n in cols: return cols[n]
        return None
    ac=pick("airport_cnt","ê³µí•­ìˆ˜","ê³µí•­_cnt")
    kc=pick("ktx_cnt","ktxìˆ˜","ktx_cnt")
    bc=pick("bus_term_cnt","ë²„ìŠ¤í„°ë¯¸ë„ìˆ˜","ë²„ìŠ¤í„°ë¯¸ë„_cnt")
    md=pick("min_dist_airport","ìµœì†Œê³µí•­ê±°ë¦¬_km","ìµœê·¼ì ‘ê³µí•­ê±°ë¦¬")
    for c in [ac,kc,bc,md]:
        if c and c in df.columns: df[c]=pd.to_numeric(df[c], errors="coerce")
    parts=[]
    if ac: parts.append(df[ac].rank(pct=True))
    if kc: parts.append(df[kc].rank(pct=True))
    if bc: parts.append(df[bc].rank(pct=True))
    if md: parts.append(1-df[md].rank(pct=True))
    if not parts: return pd.DataFrame()
    access=sum(parts)/len(parts)
    out=pd.DataFrame({"ì§€ì—­_norm":df["_sido_"], "airport_cnt":df[ac] if ac else np.nan,
                      "ktx_cnt":df[kc] if kc else np.nan, "bus_term_cnt":df[bc] if bc else np.nan,
                      "min_dist_airport":df[md] if md else np.nan, "access_score":access})
    return out.groupby("ì§€ì—­_norm",as_index=False).mean()

@st.cache_data(show_spinner=False)
def load_ktx_counts(path):
    if not path or not os.path.exists(path): return pd.DataFrame()
    df=None
    for enc in ["utf-8","cp949","euc-kr","utf-8-sig","latin1"]:
        try: df=pd.read_csv(path, encoding=enc, low_memory=False); break
        except Exception: df=None
    if df is None or df.empty: return pd.DataFrame()
    cols={c.lower():c for c in df.columns}
    sido_col=cols.get("ì‹œë„") or cols.get("ì‹œë„ëª…") or cols.get("ê´‘ì—­ì§€ìì²´")
    if not sido_col:
        addr_col=None
        for key in ["ì£¼ì†Œ","ì†Œì¬ì§€","ì†Œì¬ì§€ì£¼ì†Œ","ì—­ì£¼ì†Œ","ì—­ì‚¬ì£¼ì†Œ","ì§€ë²ˆì£¼ì†Œ","ë„ë¡œëª…ì£¼ì†Œ","ì—­ì‚¬ ë„ë¡œëª…ì£¼ì†Œ"]:
            for c in df.columns:
                if key in c: addr_col=c; break
            if addr_col: break
        if not addr_col: return pd.DataFrame()
        sidos=df[addr_col].astype(str)
        def ext(addr):
            m=re.match(r"(ì„œìš¸íŠ¹ë³„ì‹œ|ë¶€ì‚°ê´‘ì—­ì‹œ|ëŒ€êµ¬ê´‘ì—­ì‹œ|ì¸ì²œê´‘ì—­ì‹œ|ê´‘ì£¼ê´‘ì—­ì‹œ|ëŒ€ì „ê´‘ì—­ì‹œ|ìš¸ì‚°ê´‘ì—­ì‹œ|ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ|ì œì£¼íŠ¹ë³„ìì¹˜ë„|ê²½ê¸°ë„|ê°•ì›íŠ¹ë³„ìì¹˜ë„|ê°•ì›ë„|ì¶©ì²­ë¶ë„|ì¶©ì²­ë‚¨ë„|ì „ë¼ë¶ë„|ì „ë¼ë‚¨ë„|ê²½ìƒë¶ë„|ê²½ìƒë‚¨ë„)", addr.strip())
            return m.group(1) if m else ""
        sidos=sidos.map(ext)
    else:
        sidos=df[sido_col].astype(str)
    ktx=pd.DataFrame({"ì§€ì—­_norm":sidos.map(normalize_region_name)})
    ktx=ktx[ktx["ì§€ì—­_norm"].astype(str).str.len()>0]
    return ktx.value_counts("ì§€ì—­_norm").rename("ktx_cnt").reset_index()

@st.cache_data(show_spinner=False)
def load_coworking(path):
    if not path: return pd.DataFrame()
    df=None
    for enc in ["utf-8","cp949","euc-kr","utf-8-sig","latin1"]:
        try: df=pd.read_csv(path, encoding=enc, low_memory=False); break
        except Exception: df=None
    if df is None or df.empty: return pd.DataFrame()
    cols_lower={c.lower():c for c in df.columns}
    sido_col=None
    for k in ["ì‹œë„","ì‹œë„ëª…","ê´‘ì—­ì§€ìì²´","sido"]:
        if k in df.columns: sido_col=k; break
        if k in cols_lower: sido_col=cols_lower[k]; break
    if not sido_col:
        addr_col=None
        for key in ["ì£¼ì†Œ","ì†Œì¬ì§€","ë„ë¡œëª…ì£¼ì†Œ","ìƒì„¸ì£¼ì†Œ","ì§€ë²ˆì£¼ì†Œ"]:
            for c in df.columns:
                if key in c: addr_col=c; break
            if addr_col: break
        if not addr_col: return pd.DataFrame()
        sidos=df[addr_col].astype(str)
        def ext(addr):
            m=re.match(r"(ì„œìš¸íŠ¹ë³„ì‹œ|ë¶€ì‚°ê´‘ì—­ì‹œ|ëŒ€êµ¬ê´‘ì—­ì‹œ|ì¸ì²œê´‘ì—­ì‹œ|ê´‘ì£¼ê´‘ì—­ì‹œ|ëŒ€ì „ê´‘ì—­ì‹œ|ìš¸ì‚°ê´‘ì—­ì‹œ|ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ|ì œì£¼íŠ¹ë³„ìì¹˜ë„|ê²½ê¸°ë„|ê°•ì›íŠ¹ë³„ìì¹˜ë„|ê°•ì›ë„|ì¶©ì²­ë¶ë„|ì¶©ì²­ë‚¨ë„|ì „ë¼ë¶ë„|ì „ë¼ë‚¨ë„|ê²½ìƒë¶ë„|ê²½ìƒë‚¨ë„)", addr.strip())
            return m.group(1) if m else ""
        sidos=sidos.map(ext)
    else:
        sidos=df[sido_col].astype(str)
    g=pd.DataFrame({"ì§€ì—­_norm":sidos.map(normalize_region_name)})
    g=g[g["ì§€ì—­_norm"]!=""]
    return g.value_counts("ì§€ì—­_norm").rename("coworking_sites").reset_index()

# ============================ UI ============================
st.title("ë””ì§€í„¸ ë…¸ë§ˆë“œ ì§€ì—­ ì¶”ì²œ ëŒ€ì‹œë³´ë“œ")
left, right = st.columns([2, 1])
with left:
    st.subheader("ì§€ë„ì—ì„œ ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”")

# -------- ì‚¬ì´ë“œë°”: ì¹´í…Œê³ ë¦¬ & (ëŒ€ë¶„ë¥˜) ì¸í”„ë¼ --------
st.sidebar.header("ì¶”ì²œ ì¹´í…Œê³ ë¦¬")
CATEGORY_OPTIONS = [
    "ğŸ”¥ í˜„ì¬ ì¸ê¸° ì§€ì—­",
    "ğŸ›ï¸ ìˆ™ë°• ë‹¤ì–‘ ì§€ì—­",
    "ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­",
    "ğŸ’¼ ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­",
    "ğŸ’° ì €ë ´í•œ ë¹„ìš©",
    "ğŸš€ ë¹ ë¥¸ ì¸í„°ë„·",
]
selected_category = st.sidebar.selectbox("í•˜ë‚˜ë§Œ ì„ íƒ", CATEGORY_OPTIONS, index=0)

st.sidebar.markdown("---")
st.sidebar.caption("ì£¼ë³€ ì¸í”„ë¼(ëŒ€ë¶„ë¥˜) ì„ íƒ")

medical_cb     = st.sidebar.checkbox("ğŸ§‘â€âš•ï¸ ì˜ë£Œì‹œì„¤", value=False)
convenience_cb = st.sidebar.checkbox("ğŸ›’ í¸ì˜ì‹œì„¤", value=False)
workspace_cb   = st.sidebar.checkbox("ğŸ’¼ ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤", value=False)
leisure_cb     = st.sidebar.checkbox("ğŸ½ ì—¬ê°€Â·ìš´ë™", value=False)
lodging_cb     = st.sidebar.checkbox("ğŸ¨ ìˆ™ë°•", value=False)

# ëŒ€ë¶„ë¥˜ â†’ ë‚´ë¶€ ì§€í‘œ ë§¤í•‘
cb_infra_hosp    = medical_cb
cb_infra_pharm   = medical_cb
cb_infra_conv    = convenience_cb
cb_infra_laundry = convenience_cb
cb_infra_cafe    = workspace_cb
cb_infra_lib     = workspace_cb
cb_infra_pc      = leisure_cb
cb_infra_accom   = lodging_cb

# ---- ë””ë²„ê·¸ ----
st.sidebar.markdown("---")
with st.sidebar.expander("ğŸ§ª ë°ì´í„° ì§„ë‹¨/ë””ë²„ê·¸", expanded=False):
    st.write("**ê²½ë¡œ í™•ì¸**")
    st.code(f"ë°©ë¬¸ì: {file_visitors}\nì—…ì¢…ê²€ìƒ‰: {file_search_cat}\nìœ í˜•ê²€ìƒ‰: {file_search_type}")

# í•„ìš”ì‹œì—ë§Œ ë¬´ê±°ìš´ ë°ì´í„° ë¡œë”©
need_infra  = any([medical_cb, convenience_cb, workspace_cb, leisure_cb, lodging_cb]) or (selected_category=="ğŸ’¼ ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­")
need_access = (selected_category=="ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­")
need_cowork = (selected_category=="ğŸ’¼ ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­")

infra_df = pd.DataFrame()
if need_infra:
    infra_sources = resolve_infra_sources()
    infra_df = build_infra_from_sources(infra_sources)
    if not infra_df.empty:
        metrics_map = metrics_map.merge(
            infra_df.add_prefix("infra__").rename(columns={"infra__sido_norm":"ì§€ì—­_norm"}),
            on="ì§€ì—­_norm", how="left"
        )

if need_access:
    TRANSPORT_FILE = find_optional_file(["transport_access.csv","êµí†µì ‘ê·¼ì„±.csv"])
    transport_df = load_transport(TRANSPORT_FILE)
    ktx_df = load_ktx_counts(find_optional_file([
        "í•œêµ­ì² ë„ê³µì‚¬_KTX ë…¸ì„ ë³„ ì—­ì •ë³´_20240411.csv","KTX_ë…¸ì„ ë³„_ì—­ì •ë³´.csv","ktx_stations.csv"
    ]))
    if not transport_df.empty:
        metrics_map = metrics_map.merge(transport_df, on="ì§€ì—­_norm", how="left")
    if not ktx_df.empty:
        metrics_map = metrics_map.merge(ktx_df, on="ì§€ì—­_norm", how="left")
    parts=[]
    if "airport_cnt" in metrics_map:      parts.append(metrics_map["airport_cnt"].rank(pct=True))
    if "ktx_cnt" in metrics_map:          parts.append(metrics_map["ktx_cnt"].rank(pct=True))
    if "bus_term_cnt" in metrics_map:     parts.append(metrics_map["bus_term_cnt"].rank(pct=True))
    if "min_dist_airport" in metrics_map: parts.append(1 - metrics_map["min_dist_airport"].rank(pct=True))
    metrics_map["access_score"] = pd.concat(parts,axis=1).mean(axis=1).clip(0,1) if parts else np.nan

if need_cowork:
    cw_file = find_optional_file([
        "KC_CNRS_OFFM_FCLTY_DATA_2023.csv","ê³µìœ ì˜¤í”¼ìŠ¤.csv","coworking_sites.csv",
        "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ì§„í¥ê³µë‹¨_ê³µìœ ì˜¤í”¼ìŠ¤_ìš´ì˜í˜„í™©.csv","í•œêµ­ë¬¸í™”ì •ë³´ì›_ì „êµ­ê³µìœ ì˜¤í”¼ìŠ¤ì‹œì„¤.csv","ì „êµ­_ê³µìœ _ì˜¤í”¼ìŠ¤_ì‹œì„¤_ë°ì´í„°.csv"
    ])
    cow_df = load_coworking(cw_file)
    if not cow_df.empty:
        if need_infra and "infra__total_places" in metrics_map.columns:
            base = metrics_map[["ì§€ì—­_norm","infra__total_places"]]
            cow = cow_df.merge(base, on="ì§€ì—­_norm", how="left")
            cow["cowork_per10k"] = (cow["coworking_sites"]/cow["infra__total_places"].replace(0,np.nan)*10000).round(3)
            v = pd.to_numeric(cow["cowork_per10k"], errors="coerce")
        else:
            cow = cow_df.copy()
            cow["cowork_per10k"]=np.nan
            v = pd.to_numeric(cow["coworking_sites"], errors="coerce")
        rng=(v.max()-v.min())
        cow["cowork_norm"]=((v-v.min())/rng).fillna(0) if rng>0 else (v*0)
        metrics_map = metrics_map.merge(cow[["ì§€ì—­_norm","coworking_sites","cowork_per10k","cowork_norm"]], on="ì§€ì—­_norm", how="left")

# ----------------------------- ë£°/ë³´ë„ˆìŠ¤ -----------------------------
def _compute_bonus_columns(g, selected_category):
    CAT_BONUS   = 0.15
    INFRA_BONUS = 0.10
    q_vis_hi = g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].dropna().quantile(0.70) if g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].notna().any() else 1.0
    q_lod_hi = g["ìˆ™ë°•_ë¹„ì¤‘_norm"].dropna().quantile(0.70)     if g["ìˆ™ë°•_ë¹„ì¤‘_norm"].notna().any()     else 1.0
    q_acc_hi = g["access_score"].dropna().quantile(0.70)        if "access_score" in g and g["access_score"].notna().any() else 1.0
    q_cwk_hi = g["cowork_norm"].dropna().quantile(0.70)         if "cowork_norm" in g and g["cowork_norm"].notna().any()   else 1.0

    bonus = np.zeros(len(g), dtype=float)

    def add_above(series, q):
        s=pd.to_numeric(series, errors="coerce").fillna(0)
        return (s - q).clip(lower=0)

    if selected_category == "ğŸ”¥ í˜„ì¬ ì¸ê¸° ì§€ì—­":
        bonus += CAT_BONUS * add_above(g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"], q_vis_hi)
    elif selected_category == "ğŸ›ï¸ ìˆ™ë°• ë‹¤ì–‘ ì§€ì—­":
        bonus += CAT_BONUS * add_above(g["ìˆ™ë°•_ë¹„ì¤‘_norm"], q_lod_hi)
    elif selected_category == "ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­" and "access_score" in g:
        bonus += CAT_BONUS * add_above(g["access_score"], q_acc_hi)
    elif selected_category == "ğŸ’¼ ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­" and "cowork_norm" in g:
        bonus += CAT_BONUS * add_above(g["cowork_norm"], q_cwk_hi)
    elif selected_category == "ğŸ’° ì €ë ´í•œ ë¹„ìš©" and "cost_index" in g:
        rng=(g["cost_index"].max()-g["cost_index"].min())+1e-9
        bonus += CAT_BONUS * (1 - ((g["cost_index"]-g["cost_index"].min())/rng)).fillna(0)
    elif selected_category == "ğŸš€ ë¹ ë¥¸ ì¸í„°ë„·" and "internet_mbps" in g:
        rng=(g["internet_mbps"].max()-g["internet_mbps"].min())+1e-9
        bonus += CAT_BONUS * (((g["internet_mbps"]-g["internet_mbps"].min())/rng)).fillna(0)

    # ì¸í”„ë¼ ë³´ë„ˆìŠ¤(ëŒ€ë¶„ë¥˜ ì²´í¬ â†’ ë‚´ë¶€ ì§€í‘œì— ì¼ê´„ ê°€ì‚°)
    def has(col): return col in g.columns and pd.api.types.is_numeric_dtype(g[col])
    infra_cols = {
        "cafe":"infra__cafe_count_norm", "conv":"infra__convenience_count_norm",
        "accom":"infra__accommodation_count_norm", "hosp":"infra__hospital_count_norm",
        "pharm":"infra__pharmacy_count_norm", "pc":"infra__pc_cafe_count_norm",
        "laundry":"infra__laundry_count_norm", "lib":"infra__library_museum_count_norm",
    }
    if cb_infra_hosp and has(infra_cols["hosp"]):   bonus += INFRA_BONUS * g[infra_cols["hosp"]].fillna(0)
    if cb_infra_pharm and has(infra_cols["pharm"]): bonus += INFRA_BONUS * g[infra_cols["pharm"]].fillna(0)
    if cb_infra_conv and has(infra_cols["conv"]):       bonus += INFRA_BONUS * g[infra_cols["conv"]].fillna(0)
    if cb_infra_laundry and has(infra_cols["laundry"]): bonus += INFRA_BONUS * g[infra_cols["laundry"]].fillna(0)
    if cb_infra_cafe and has(infra_cols["cafe"]): bonus += INFRA_BONUS * g[infra_cols["cafe"]].fillna(0)
    if cb_infra_lib  and has(infra_cols["lib"]):  bonus += INFRA_BONUS * g[infra_cols["lib"]].fillna(0)
    if cb_infra_pc   and has(infra_cols["pc"]):   bonus += INFRA_BONUS * g[infra_cols["pc"]].fillna(0)
    if cb_infra_accom and has(infra_cols["accom"]): bonus += INFRA_BONUS * g[infra_cols["accom"]].fillna(0)

    return bonus

def apply_category_rules_all(df):
    g = df.copy()
    bonus = _compute_bonus_columns(g, selected_category)
    g["NSI"] = (g["NSI_base"] + bonus).clip(0,1)
    return g

def apply_category_rules(df):
    g = df.copy()
    q_vis_hi = g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].dropna().quantile(0.70) if g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"].notna().any() else 1.0
    q_lod_hi = g["ìˆ™ë°•_ë¹„ì¤‘_norm"].dropna().quantile(0.70)     if g["ìˆ™ë°•_ë¹„ì¤‘_norm"].notna().any()     else 1.0
    q_acc_hi = g["access_score"].dropna().quantile(0.70)        if "access_score" in g and g["access_score"].notna().any() else 1.0
    q_cwk_hi = g["cowork_norm"].dropna().quantile(0.70)         if "cowork_norm" in g and g["cowork_norm"].notna().any()   else 1.0

    if selected_category == "ğŸ”¥ í˜„ì¬ ì¸ê¸° ì§€ì—­":
        mask = (g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"] >= q_vis_hi)
    elif selected_category == "ğŸ›ï¸ ìˆ™ë°• ë‹¤ì–‘ ì§€ì—­":
        mask = (g["ìˆ™ë°•_ë¹„ì¤‘_norm"] >= q_lod_hi)
    elif selected_category == "ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­" and "access_score" in g:
        mask = (g["access_score"] >= q_acc_hi)
    elif selected_category == "ğŸ’¼ ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­" and "cowork_norm" in g:
        mask = (g["cowork_norm"] >= q_cwk_hi)
    elif selected_category == "ğŸ’° ì €ë ´í•œ ë¹„ìš©" and "cost_index" in g:
        mask = (g["cost_index"] <= g["cost_index"].quantile(0.30))
    elif selected_category == "ğŸš€ ë¹ ë¥¸ ì¸í„°ë„·" and "internet_mbps" in g:
        mask = (g["internet_mbps"] >= g["internet_mbps"].quantile(0.70))
    else:
        mask = pd.Series(True, index=g.index)

    g = g.loc[mask].copy()
    bonus = _compute_bonus_columns(g, selected_category)
    g["NSI"] = (g["NSI_base"] + bonus).clip(0,1)
    return g

metrics_all = apply_category_rules_all(metrics_map)     # ì „ì²´(íˆ´íŒ)
metrics_after_rules = apply_category_rules(metrics_map) # í•„í„°ë§ìš©

# ---------- ì¹´í…Œê³ ë¦¬ë³„ í‘œì‹œ ì ìˆ˜(display_score) & ë­í‚¹ ----------
def normalized(series, invert=False):
    s = pd.to_numeric(series, errors="coerce")
    mm = minmax(s)
    return (1-mm) if invert else mm

def category_display_score(df, category):
    g = df.copy()
    score = None
    if category == "ğŸ”¥ í˜„ì¬ ì¸ê¸° ì§€ì—­":
        score = g["ë°©ë¬¸ì_ì ìœ ìœ¨_norm"]
    elif category == "ğŸ›ï¸ ìˆ™ë°• ë‹¤ì–‘ ì§€ì—­":
        score = g["ìˆ™ë°•_ë¹„ì¤‘_norm"]
    elif category == "ğŸš‰ êµí†µ ì¢‹ì€ ì§€ì—­":
        if "access_score" in g:
            score = normalized(g["access_score"])
        elif "ktx_cnt" in g:
            score = g["ktx_cnt"].rank(pct=True)  # ëŒ€ì²´
    elif category == "ğŸ’¼ ì½”ì›Œí‚¹ ì¸í”„ë¼ í’ë¶€ ì§€ì—­":
        if "cowork_norm" in g:
            score = g["cowork_norm"]
        elif "coworking_sites" in g:
            score = g["coworking_sites"].rank(pct=True)
    elif category == "ğŸ’° ì €ë ´í•œ ë¹„ìš©" and "cost_index" in g:
        score = normalized(g["cost_index"], invert=True)
    elif category == "ğŸš€ ë¹ ë¥¸ ì¸í„°ë„·" and "internet_mbps" in g:
        score = normalized(g["internet_mbps"])
    # í´ë°±: ì „ë¶€ ì—†ê±°ë‚˜ ë³€ë³„ë ¥ì´ ì—†ìœ¼ë©´ NSI ì‚¬ìš©
    if score is None or pd.to_numeric(score, errors="coerce").fillna(0).nunique() <= 1:
        score = g["NSI"]
    return pd.to_numeric(score, errors="coerce").fillna(0).clip(0,1)

ranked_all = metrics_all.copy()
ranked_all["NSI"]  = ranked_all["NSI"].fillna(ranked_all["NSI_base"]).fillna(0.0).clip(0,1)

# ì´ ë·°ê°€ ì§€ë„/í•˜ì´ë¼ì´íŠ¸ì— ì‚¬ìš©ë¨
ranked_view = ranked_all.copy()
ranked_view["display_score"] = category_display_score(ranked_all, selected_category)
ranked_view["rank_view"]     = ranked_view["display_score"].rank(ascending=False, method="min").astype(int)

# =============================== ì§€ë„ ===============================
COLOR_TOP1, COLOR_TOP2, COLOR_TOP3 = "#e60049", "#ffd43b", "#4dabf7"
COLOR_SEL, COLOR_BASE = "#51cf66", "#cfd4da"

def pick_color(region_norm, selected_region_norm=None):
    if selected_region_norm and region_norm == selected_region_norm:
        return COLOR_SEL
    r_series = ranked_view.loc[ranked_view["ì§€ì—­_norm"]==region_norm, "rank_view"]
    r = int(r_series.min()) if not r_series.empty else 999
    return {1:COLOR_TOP1, 2:COLOR_TOP2, 3:COLOR_TOP3}.get(r, COLOR_BASE)

MAP_HEIGHT = 680
with left:
    m = folium.Map(location=[36.5,127.8], zoom_start=7, tiles="cartodbpositron", prefer_canvas=True)

    gj, gj_err = (None, "no_path")
    if KOREA_GEOJSON and os.path.exists(KOREA_GEOJSON):
        gj, gj_err = load_geojson_safe(KOREA_GEOJSON)

    coords_df = pd.DataFrame([{"ì§€ì—­_norm":k,"lat":v[0],"lon":v[1]} for k,v in REGION_COORDS.items()])
    ranked_view = ranked_view.drop(columns=[c for c in ["lat","lon"] if c in ranked_view.columns]).merge(coords_df, on="ì§€ì—­_norm", how="left")
    rank_lookup = ranked_view.set_index("ì§€ì—­_norm")[["rank_view","display_score"]].to_dict("index")

    if gj is not None:
        for ft in gj.get("features", []):
            props = ft.get("properties", {}) or {}
            region_raw=None
            for k in GEO_PROP_KEYS:
                if k in props and props[k]: region_raw=props[k]; break
            if region_raw is None:
                textish=[str(v) for v in props.values() if isinstance(v,str)]
                region_raw=max(textish, key=len) if textish else ""
            rname=normalize_region_name(region_raw)
            stats=rank_lookup.get(rname)
            props.update({
                "REGION_NAME": rname,
                "RANK_TXT": f"{int(stats['rank_view'])}ìœ„" if stats else "-",
                "SCORE_TXT": f"{float(stats['display_score']):.3f}" if stats else "-"
            })
            ft["properties"]=props

        def style_function(feature):
            rname = feature["properties"].get("REGION_NAME","")
            color = pick_color(rname, normalize_region_name(st.session_state.get("selected_region")))
            return {"fillColor":color, "color":color, "weight":1, "fillOpacity":0.70, "opacity":0.9}

        def highlight_function(feature):
            return {"fillOpacity":0.92, "weight":2}

        tooltip_css = (
            "background-color: rgba(28, 45, 28, 0.92); color: #fff; "
            "font-size: 12px; padding: 6px 8px; border-radius: 6px; "
            "white-space: nowrap; border: 0.5px solid rgba(255,255,255,0.15);"
        )

        GeoJson(
            gj,
            name="regions",
            style_function=style_function,
            highlight_function=highlight_function,
            smooth_factor=1.0,
            tooltip=GeoJsonTooltip(
                fields=["REGION_NAME","RANK_TXT","SCORE_TXT"],
                aliases=["ì§€ì—­","ë­í‚¹","ì ìˆ˜"],
                labels=True, sticky=True, style=tooltip_css
            ),
            popup=GeoJsonPopup(fields=["REGION_NAME"], labels=False),
        ).add_to(m)

        legend_html = f"""
        <div style="
          position: fixed; bottom: 20px; left: 20px; z-index: 9999;
          background: rgba(255,255,255,0.96); padding: 12px 14px; border-radius: 10px;
          box-shadow: 0 2px 10px rgba(0,0,0,0.15); font-size: 13px; color: #222;">
          <div style="font-weight:700; margin-bottom:8px;">ë­í‚¹ í‘œê¸°</div>
          <div style="display:flex; align-items:center; gap:8px; margin:4px 0;">
            <span style="display:inline-block;width:14px;height:14px;background:{COLOR_TOP1};
                         border:1px solid rgba(0,0,0,.2);border-radius:3px;"></span><span>1ìœ„</span>
          </div>
          <div style="display:flex; align-items:center; gap:8px; margin:4px 0;">
            <span style="display:inline-block;width:14px;height:14px;background:{COLOR_TOP2};
                         border:1px solid rgba(0,0,0,.2);border-radius:3px;"></span><span>2ìœ„</span>
          </div>
          <div style="display:flex; align-items:center; gap:8px; margin:4px 0;">
            <span style="display:inline-block;width:14px;height:14px;background:{COLOR_TOP3};
                         border:1px solid rgba(0,0,0,.2);border-radius:3px;"></span><span>3ìœ„</span>
          </div>
        </div>"""
        m.get_root().html.add_child(folium.Element(legend_html))
    else:
        for _, r in ranked_view.iterrows():
            if pd.isna(r.get("lat")) or pd.isna(r.get("lon")): continue
            color = pick_color(r["ì§€ì—­_norm"], normalize_region_name(st.session_state.get("selected_region")))
            score = float(r["display_score"])
            text = f"ì§€ì—­&nbsp;&nbsp;{r['ì§€ì—­_norm']}<br>ë­í‚¹&nbsp;{int(r['rank_view'])}ìœ„<br>ì ìˆ˜&nbsp;&nbsp;&nbsp;{score:.3f}"
            folium.CircleMarker(
                location=[r["lat"], r["lon"]], radius=6+14*score,
                color=color, fill=True, fill_color=color,
                fill_opacity=0.78, opacity=0.95, weight=1
            ).add_child(folium.Tooltip(text, sticky=True, direction="top")).add_to(m)

    def extract_clicked_name(state):
        if not state: return None
        nm = state.get("last_object_clicked_popup")
        if nm: return normalize_region_name(str(nm))
        obj = state.get("last_active_drawing") or {}
        if isinstance(obj, dict):
            nm = obj.get("properties", {}).get("REGION_NAME")
            if nm: return normalize_region_name(str(nm))
        obj = state.get("last_object_clicked") or {}
        if isinstance(obj, dict):
            nm = obj.get("popup") or (obj.get("properties", {}) or {}).get("REGION_NAME")
            if nm: return normalize_region_name(str(nm))
        return None

    map_state = st_folium(m, width=None, height=MAP_HEIGHT, key="main_map")
    clicked_name = extract_clicked_name(map_state)
    if clicked_name and clicked_name != st.session_state.get("_last_clicked"):
        st.session_state.selected_region = clicked_name
        st.session_state._last_clicked = clicked_name

# ============================ ìš°ì¸¡ íŒ¨ë„ ============================
with right:
    st.subheader("ì»¤ë®¤ë‹ˆí‹°")

    # ì§€ì—­ í•˜ì´ë¼ì´íŠ¸: í° í…ìŠ¤íŠ¸ë§Œ(ì„¤ëª… ì œê±°) + ì¹´í…Œê³ ë¦¬ ì ìˆ˜ ê¸°ì¤€ Top5
    st.markdown("### ì§€ì—­ í•˜ì´ë¼ì´íŠ¸")
    if st.session_state.get("selected_region"):
        sel_name = normalize_region_name(st.session_state.selected_region)
        sel = ranked_view.loc[ranked_view["ì§€ì—­_norm"]==sel_name]
        if not sel.empty:
            r=sel.iloc[0]
            st.info(f"**ì„ íƒ ì§€ì—­: {r['ì§€ì—­_norm']}** â€” {int(r['rank_view'])}ìœ„ Â· ì ìˆ˜ {float(r['display_score']):.3f}")

    for _, r in ranked_view.sort_values("display_score", ascending=False).head(5).iterrows():
        name = r["ì§€ì—­_norm"]
        strong = "**" if st.session_state.get("selected_region") and normalize_region_name(st.session_state.selected_region)==name else ""
        st.write(f"{strong}{name}{strong} â€” {int(r['rank_view'])}ìœ„ Â· ì ìˆ˜ {float(r['display_score']):.3f}")

    # QnA/ê²Œì‹œíŒ(ê°„ë‹¨)
    st.markdown("### QnA Â· ê²Œì‹œíŒ")
    STORE_PATH = os.path.join(DATA_DIR, "community_qna.json")
    def load_store():
        try:
            with open(STORE_PATH,"r",encoding="utf-8") as f: return json.load(f)
        except Exception: return {"posts":[]}
    def save_store(data):
        try:
            with open(STORE_PATH,"w",encoding="utf-8") as f: json.dump(data,f,ensure_ascii=False,indent=2)
        except Exception:
            st.warning("ê²Œì‹œê¸€ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì“°ê¸° ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")
    if "qna_store" not in st.session_state: st.session_state.qna_store = load_store()
    store = st.session_state.qna_store

    tabs = st.tabs(["ì§ˆë¬¸ ì˜¬ë¦¬ê¸°(QnA)", "ê¸€ì“°ê¸°(ê²Œì‹œíŒ)", "í”¼ë“œ ë³´ê¸°"])
    with tabs[0]:
        with st.form("form_qna"):
            title = st.text_input("ì œëª©", value="")
            content = st.text_area("ë‚´ìš©", height=120, value="")
            region_tag = st.text_input("ê´€ë ¨ ì§€ì—­(ì„ íƒ, ì˜ˆ: ì œì£¼Â·ê°•ì›)", value=st.session_state.selected_region or "")
            submit = st.form_submit_button("ì§ˆë¬¸ ë“±ë¡")
        if submit and title.strip():
            store["posts"].append({
                "id": str(uuid.uuid4()), "type":"qna",
                "title": title.strip(), "content": content.strip(),
                "region": normalize_region_name(region_tag) if region_tag else "",
                "author":"ìµëª…", "created": int(time.time()), "answers":[]
            }); 
            save_store(store); st.success("ì§ˆë¬¸ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    with tabs[1]:
        with st.form("form_board"):
            title2 = st.text_input("ì œëª© ", value="")
            content2 = st.text_area("ë³¸ë¬¸", height=140, value="")
            region_tag2 = st.text_input("ì§€ì—­ íƒœê·¸(ì„ íƒ)", value=st.session_state.selected_region or "")
            submit2 = st.form_submit_button("ê¸€ ë“±ë¡")
        if submit2 and title2.strip():
            store["posts"].append({
                "id": str(uuid.uuid4()), "type":"board",
                "title": title2.strip(), "content": content2.strip(),
                "region": normalize_region_name(region_tag2) if region_tag2 else "",
                "author":"ìµëª…", "created": int(time.time()), "comments":[]
            })
            save_store(store); st.success("ê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    with tabs[2]:
        c1, c2 = st.columns([1,1])
        with c1: feed_type = st.multiselect("ìœ í˜•", ["qna","board"], default=["qna","board"])
        with c2: feed_region = st.text_input("ì§€ì—­ í•„í„°(ë¶€ë¶„ì¼ì¹˜)", value="")
        posts=[p for p in store["posts"] if p["type"] in feed_type]
        if feed_region.strip():
            key=normalize_region_name(feed_region)
            posts=[p for p in posts if key in normalize_region_name(p.get("region",""))]
        posts=sorted(posts, key=lambda p:p.get("created",0), reverse=True)
        for p in posts:
            with st.expander(f"[{'QnA' if p['type']=='qna' else 'ê²Œì‹œê¸€'}] {p['title']}  Â·  {p.get('region','') or 'ì „ì²´'}"):
                st.write(p["content"] or "(ë‚´ìš© ì—†ìŒ)")
                if p["type"]=="qna":
                    for a in p.get("answers", []):
                        st.markdown(f"- **ë‹µë³€**: {a['content']}  â€” _{a.get('author','ìµëª…')}_")
                    with st.form(f"ans_{p['id']}"):
                        ans = st.text_input("ë‹µë³€ ë‹¬ê¸°", value="")
                        if st.form_submit_button("ë“±ë¡") and ans.strip():
                            p.setdefault("answers",[]).append({"content":ans.strip(),"author":"ìµëª…","created":int(time.time())})
                            save_store(store); st.success("ë‹µë³€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    for cmt in p.get("comments", []):
                        st.markdown(f"- **ëŒ“ê¸€**: {cmt['content']}  â€” _{cmt.get('author','ìµëª…')}_")
                    with st.form(f"cmt_{p['id']}"):
                        cmt = st.text_input("ëŒ“ê¸€ ë‹¬ê¸°", value="")
                        if st.form_submit_button("ë“±ë¡") and cmt.strip():
                            p.setdefault("comments",[]).append({"content":cmt.strip(),"author":"ìµëª…","created":int(time.time())})
                            save_store(store); st.success("ëŒ“ê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ============================ ë­í‚¹/ë‹¤ìš´ë¡œë“œ ============================
st.subheader("ì¶”ì²œ ë­í‚¹ (Top 5)")
cols_to_show = ["ê´‘ì—­ì§€ìì²´ëª…","display_score","NSI","NSI_base","ë°©ë¬¸ììˆ˜_í•©ê³„","ë°©ë¬¸ì_ì ìœ ìœ¨","ìˆ™ë°•_ì§€ì¶œë¹„ì¤‘(%)"]
if "access_score" in metrics_map.columns and metrics_map["access_score"].notna().any():
    cols_to_show += ["access_score","ktx_cnt"]
if "coworking_sites" in metrics_map.columns:
    cols_to_show += ["coworking_sites","cowork_per10k"]
if 'infra__cafe_count_per10k' in metrics_after_rules.columns:
    cols_to_show += [
        "infra__cafe_count_per10k","infra__convenience_count_per10k","infra__accommodation_count_per10k",
        "infra__hospital_count_per10k","infra__pharmacy_count_per10k"
    ]

# í˜„ì¬ ë·° ê¸°ì¤€ ì •ë ¬
rec = ranked_view.sort_values("display_score", ascending=False)[[c for c in cols_to_show if c in ranked_view.columns]]
top5 = rec.head(5)

out = rec.copy()
if "ë°©ë¬¸ììˆ˜_í•©ê³„" in out.columns: out["ë°©ë¬¸ììˆ˜_í•©ê³„"] = out["ë°©ë¬¸ììˆ˜_í•©ê³„"].fillna(0).astype(int)
for c in out.columns:
    if c not in ["ê´‘ì—­ì§€ìì²´ëª…"]:
        out[c] = pd.to_numeric(out[c], errors="coerce").round(4)

st.dataframe(top5.reset_index(drop=True), use_container_width=True)
st.download_button("â¬‡ï¸ ì „ì²´ ë­í‚¹ CSV ì €ì¥", out.to_csv(index=False).encode("utf-8-sig"),
                   file_name="ranking_full.csv", mime="text/csv")

# ============================ í‚¤ì›Œë“œ Â· ì¹´í…Œê³ ë¦¬ íƒìƒ‰ ============================
st.markdown("## í‚¤ì›Œë“œ Â· ì¹´í…Œê³ ë¦¬ íƒìƒ‰")
search_cat_df, search_cat_cols   = load_search_counts(file_search_cat)
search_type_df, search_type_cols = load_search_counts(file_search_type)

def render_search_chart(df, cols, title_key, default_regions=None, key_prefix="cat"):
    rcol, gcol, vcol = cols
    if df.empty or not (rcol and gcol and vcol):
        st.info("ê²€ìƒ‰ê±´ìˆ˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (íŒŒì¼ ë˜ëŠ” ì»¬ëŸ¼ í™•ì¸)")
        return
    regions = sorted(df[rcol].dropna().astype(str).map(normalize_region_name).unique().tolist())
    if not regions:
        st.info("ì§€ì—­ ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return
    with st.container():
        c1, c2 = st.columns([2,1])
        default = default_regions or ([st.session_state.selected_region] if st.session_state.get("selected_region") else regions)
        with c1:
            pick_regions = st.multiselect("ì§€ì—­ ì„ íƒ", options=regions,
                                          default=default, key=f"{key_prefix}_regions")
        with c2:
            topn = st.slider("ìƒìœ„ N", min_value=5, max_value=30, value=5, step=1, key=f"{key_prefix}_topn")
        temp = df.copy()
        temp["_ì§€ì—­_"] = temp[rcol].astype(str).map(normalize_region_name)
        temp[vcol] = pd.to_numeric(temp[vcol], errors="coerce").fillna(0)
        temp = temp[temp["_ì§€ì—­_"].isin([normalize_region_name(r) for r in pick_regions])]
        grp = (temp.groupby(gcol, as_index=False)[vcol].sum()
                    .sort_values(vcol, ascending=False).head(topn))
        st.bar_chart(grp.set_index(gcol)[vcol])

tabs_kc = st.tabs(["ì—…ì¢…/ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ê±´ìˆ˜", "ìœ í˜•/í‚¤ì›Œë“œ ê²€ìƒ‰ê±´ìˆ˜"])
with tabs_kc[0]:
    render_search_chart(search_cat_df, search_cat_cols, "ì¹´í…Œê³ ë¦¬", key_prefix="scat")
with tabs_kc[1]:
    render_search_chart(search_type_df, search_type_cols, "í‚¤ì›Œë“œ", key_prefix="stype")

# ì„ íƒ ì§€ì—­ ì „ìš© Top5(ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ)
st.markdown("### ì„ íƒ ì§€ì—­ ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ Top 5")
def topn_by_region(df, cols, region_norm, topn=5):
    rcol, gcol, vcol = cols
    if df.empty or not (rcol and gcol and vcol):
        return pd.DataFrame(), None, None
    temp = df.copy()
    temp["_ì§€ì—­_"] = temp[rcol].astype(str).map(normalize_region_name)
    temp[vcol] = pd.to_numeric(temp[vcol], errors="coerce")
    temp = temp[temp["_ì§€ì—­_"]==region_norm]
    if temp.empty: return pd.DataFrame(), None, None
    grp = (temp.groupby(gcol, as_index=False)[vcol].sum()
                .sort_values(vcol, ascending=False).head(topn))
    return grp, gcol, vcol

if st.session_state.get("selected_region"):
    _sel = normalize_region_name(st.session_state.selected_region)
    c1, c2 = st.columns(2)
    with c1:
        grp, gcol, vcol = topn_by_region(search_cat_df, search_cat_cols, _sel, topn=5)
        if grp.empty:
            st.info("ì„ íƒ ì§€ì—­ì˜ ì—…ì¢…/ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.subheader("ì—…ì¢…/ì¹´í…Œê³ ë¦¬ Top 5")
            st.bar_chart(grp.set_index(gcol)[vcol])
    with c2:
        grp2, gcol2, vcol2 = topn_by_region(search_type_df, search_type_cols, _sel, topn=5)
        if grp2.empty:
            st.info("ì„ íƒ ì§€ì—­ì˜ ìœ í˜•/í‚¤ì›Œë“œ ê²€ìƒ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.subheader("ìœ í˜•/í‚¤ì›Œë“œ Top 5")
            st.bar_chart(grp2.set_index(gcol2)[vcol2])
else:
    st.caption("ì§€ì—­ì„ ì„ íƒí•˜ë©´ í•´ë‹¹ ì§€ì—­ì˜ ìƒìœ„ ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œë¥¼ 5ê°œ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.")

# ----------------------------- ì¶œì²˜ -----------------------------
st.markdown("""
---
**ë°ì´í„° ì¶œì²˜**  
- í•œêµ­ê´€ê´‘ë°ì´í„°ë©: ì§€ì—­ë³„ ë°©ë¬¸ììˆ˜, ì§€ì—­ë³„ ê´€ê´‘ì§€ì¶œì•¡, ì§€ì—­ë³„ ê²€ìƒ‰ê±´ìˆ˜, ì¸ê¸°ê´€ê´‘ì§€ í˜„í™©  
- ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨: ìƒê°€(ìƒê¶Œ) ì •ë³´  
- í•œêµ­ì² ë„ê³µì‚¬: KTX ë…¸ì„ ë³„ ì—­ì •ë³´  
- í•œêµ­ë¬¸í™”ì •ë³´ì›: ì „êµ­ê³µìœ ì˜¤í”¼ìŠ¤ì‹œì„¤ë°ì´í„°
""")
